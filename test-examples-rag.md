make test-examples-rag

ğŸ¤– [36mRAG Conversation System Examples[0m
[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m

[33mPrerequisites:[0m
  1. ekoDB server running (make run in ekodb/)
  2. OPENAI_API_KEY set in server environment
  3. API_BASE_URL and API_BASE_KEY exported in your shell

[36mBuilding Rust client library...[0m
âœ“ Rust client built

[36mBuilding Python client bindings...[0m
ğŸ¹ Building a mixed python/rust project
ğŸ”— Found pyo3 bindings with abi3 support
ğŸ Not using a specific python interpreter
ğŸ’» Using `MACOSX_DEPLOYMENT_TARGET=11.0` for aarch64-apple-darwin by default
ğŸ“¦ Built wheel for abi3 Python â‰¥ 3.8 to /Users/tek/Development/ekoDB/ekodb-client/ekodb-client-py/target/wheels/ekodb_client-0.7.1-cp38-abi3-macosx_11_0_arm64.whl
âœ“ Python client built and installed

[36mBuilding TypeScript client library...[0m
âœ“ TypeScript client built

[36mInstalling TypeScript client in examples...[0m
âœ“ TypeScript client installed

[36mBuilding TypeScript example...[0m
âœ“ TypeScript example built

[36mBuilding Go client library...[0m
âœ“ Go client built

[36mBuilding Go RAG example...[0m
âœ“ Go example built

[36mBuilding Kotlin client library...[0m
âœ“ Kotlin client built

[36mBuilding Kotlin RAG example...[0m
âœ“ Kotlin example built

[36mRunning Rust RAG Example...[0m
[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m
=== ekoDB RAG Conversation System ===

This example shows how ekoDB can power a self-improving AI system
that learns from its own conversation history.

=== Step 1: Building Conversation History ===
Storing previous conversations with embeddings...

  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 34 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 490.391959ms
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 169 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 504.572958ms
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 33 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 287.452334ms
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 230 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 209.763084ms
    â€¢ Function auto-cleaned up by client
âœ“ Stored Rust programming conversation (4 messages)
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 31 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 398.472625ms
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 217 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 288.644209ms
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 33 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 615.235458ms
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 232 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 503.225375ms
    â€¢ Function auto-cleaned up by client
âœ“ Stored database design conversation (4 messages)
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 36 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 370.245167ms
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 178 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 274.402042ms
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 37 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 308.598875ms
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 213 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 299.113584ms
    â€¢ Function auto-cleaned up by client
âœ“ Stored performance optimization conversation (4 messages)

=== Step 2: New User Question with Context Retrieval ===
User asks: "How do I write memory-safe high-performance database code?"

=== Step 3: Searching Related Context ===
Using hybrid search to find relevant messages from all conversations...


â†’ Generating embedding for user question...
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 58 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 945.135791ms
    â€¢ Function auto-cleaned up by client

â†’ Executing hybrid_search()...
  â€¢ Collection: rag_messages
  â€¢ Query text: "How do I write memory-safe high-performance database code?"
  â€¢ Vector dimensions: 1536
  â€¢ Limit: 5 results
  â€¢ Search type: Semantic (vector) + Keyword (text)
  â€¢ Server combines both scores for relevance ranking
  âœ“ Search completed in 100.799917ms

âœ“ Found 5 related messages across all conversations:
  1. From conv_database_design
     Use NoSQL when you need: flexible schemas, horizontal scaling, high write throughput, or when working with unstructured data. SQL is better for complex queries, ACID transactions, and structured data with well-defined relationships.

  2. From conv_database_design
     What is database normalization?

  3. From conv_database_design
     Database normalization is the process of organizing data to reduce redundancy and improve data integrity. It involves dividing large tables into smaller ones and defining relationships between them using foreign keys.

  4. From conv_database_design
     When should I use NoSQL over SQL?

  5. From conv_performance
     How can I optimize database queries?

=== Step 4: Generating Context-Aware Response ===
âœ“ AI Response (with context from 3 conversations):

Writing memory-safe, high-performance database code involves several practices:

1. **Normalization**: As discussed in context 2 and 3, normalizing your database is important to reduce redundancy and improve data integrity. A well-organized database will inherently speed up many queries.

2. **Choose the Right Database Type**: As discussed in context 1 and 4, the choice between SQL and NoSQL can have a significant impact on performance. If your data is structured and has well-defined relationships, SQL databases can be more efficient. If you have unstructured data, or need high write throughput and horizontal scaling, NoSQL can be more performant.

3. **Indexing**: Indexes can dramatically improve query performance by reducing the number of disk I/O operations. However, indexes require additional disk space and can slow write operations, so it's important to use them judiciously.

4. **Optimized Queries**: Write your queries in such a way as to minimize the amount of data that has to be read from the disk. Avoid using SELECT *, as it requires more memory to store the results. Instead, specify the exact columns you need. Also, use WHERE clauses to filter data as early as possible in the query. 

5. **Use Prepared Statements**: Prepared statements not only help prevent SQL injection attacks, but they can also improve performance by allowing the database to cache the query execution plan and reuse it across multiple instances of the same query.

6. **Connection Pooling**: Establishing a database connection is a time-consuming process. Connection pooling allows you to reuse existing connections, reducing the overhead of establishing a new connection for every query.

7. **Profiling and Monitoring**: Use profiling tools to identify slow queries and performance bottlenecks. This will help you understand where optimizations can be made. 

In terms of memory safety, ensure you are properly closing connections and cleaning up resources after use. Also, avoid loading large amounts of data into memory at once, especially in languages that don't automatically manage memory for you. 

Please note, these are general tips. Depending on the specific database system and programming language you are using, there might be other specific practices you should follow.

=== Step 5: Storing New Conversation ===
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 58 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 516.738417ms
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 2286 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 506.1ms
    â€¢ Function auto-cleaned up by client
âœ“ New conversation stored and indexed for future retrieval

=== Step 6: Cross-Conversation Search ===
Searching for messages about 'ownership' across ALL conversations...


â†’ Executing text_search()...
  â€¢ Collection: rag_messages
  â€¢ Query: "ownership system"
  â€¢ Limit: 3 results
  â€¢ Search method: Full-text with fuzzy matching & stemming
  â€¢ No vector embeddings needed - pure keyword search
  âœ“ Text search completed in 62.196167ms

âœ“ Found 3 messages mentioning ownership:
  1. From conv_rust_programming: Rust's key features include: memory safety without garbage collection, zero-cost abstractions, ownership system, powerful type system, and excellent concurrency support.

  2. From conv_performance: Rust's ownership system provides zero-cost memory management. Use Box for heap allocation, Rc/Arc for shared ownership, and avoid cloning large data structures. The compiler optimizes away unnecessary allocations.

  3. From conv_new_question: Writing memory-safe, high-performance database code involves several practices:

1. **Normalization**: As discussed in context 2 and 3, normalizing your database is important to reduce redundancy and improve data integrity. A well-organized database will inherently speed up many queries.

2. **Choose the Right Database Type**: As discussed in context 1 and 4, the choice between SQL and NoSQL can have a significant impact on performance. If your data is structured and has well-defined relationships, SQL databases can be more efficient. If you have unstructured data, or need high write throughput and horizontal scaling, NoSQL can be more performant.

3. **Indexing**: Indexes can dramatically improve query performance by reducing the number of disk I/O operations. However, indexes require additional disk space and can slow write operations, so it's important to use them judiciously.

4. **Optimized Queries**: Write your queries in such a way as to minimize the amount of data that has to be read from the disk. Avoid using SELECT *, as it requires more memory to store the results. Instead, specify the exact columns you need. Also, use WHERE clauses to filter data as early as possible in the query. 

5. **Use Prepared Statements**: Prepared statements not only help prevent SQL injection attacks, but they can also improve performance by allowing the database to cache the query execution plan and reuse it across multiple instances of the same query.

6. **Connection Pooling**: Establishing a database connection is a time-consuming process. Connection pooling allows you to reuse existing connections, reducing the overhead of establishing a new connection for every query.

7. **Profiling and Monitoring**: Use profiling tools to identify slow queries and performance bottlenecks. This will help you understand where optimizations can be made. 

In terms of memory safety, ensure you are properly closing connections and cleaning up resources after use. Also, avoid loading large amounts of data into memory at once, especially in languages that don't automatically manage memory for you. 

Please note, these are general tips. Depending on the specific database system and programming language you are using, there might be other specific practices you should follow.

=== System Statistics ===

â†’ Querying database statistics...
  â€¢ Using find_all() helper - simplified query API

ğŸ“Š Database Statistics:
  â€¢ Total conversations: 4
  â€¢ Total messages stored: 14
  â€¢ All messages indexed for vector search âœ“
  â€¢ All messages indexed for text search âœ“
  â€¢ All messages queryable by metadata âœ“

=== Step 8: Dynamic Search Configuration ===
Each conversation can have its own search config...

ğŸ’¡ Conversations can store custom search configurations:
  â€¢ Search type: hybrid, text, or vector
  â€¢ Relevance thresholds
  â€¢ Filter by tags or metadata
  â€¢ Collection-specific settings
  â€¢ Per-conversation AI behavior

This enables context-aware search tuned to each conversation's needs!


=== Cleanup ===
Deleting example collections...

âœ… All done! RAG system demonstrated successfully.

âœ“ Using search results to enhance AI responses (RAG)
âœ“ Building a self-improving knowledge base
âœ“ Dynamic search configurations per conversation

ekoDB provides everything needed for AI-powered applications:
  â€¢ Vector search (semantic similarity)
  â€¢ Text search (keyword matching)
  â€¢ Hybrid search (best of both)
  â€¢ AI functions (Chat, Embed)
  â€¢ Flexible querying and filtering
  â€¢ All in one database - no external dependencies!


[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m
[36mRunning Python RAG Example...[0m
[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m
=== ekoDB RAG Conversation System ===

This example shows how ekoDB can power a self-improving AI system
that learns from its own conversation history.

=== Step 1: Building Conversation History ===
Storing previous conversations with embeddings...

  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 34 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.307s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 169 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.219s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 33 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.380s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 230 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.298s
    â€¢ Function auto-cleaned up by client
âœ“ Stored Rust programming conversation (4 messages)
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 31 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.411s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 217 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.259s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 33 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.438s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 232 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.307s
    â€¢ Function auto-cleaned up by client
âœ“ Stored database design conversation (4 messages)
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 36 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.593s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 178 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.310s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 37 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.593s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 213 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.294s
    â€¢ Function auto-cleaned up by client
âœ“ Stored performance optimization conversation (4 messages)

=== Step 2: New User Question with Context Retrieval ===
User asks: "How do I write memory-safe high-performance database code?"

=== Step 3: Searching Related Context ===
Using hybrid search to find relevant messages from all conversations...


â†’ Generating embedding for user question...
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 58 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.304s
    â€¢ Function auto-cleaned up by client

â†’ Executing hybrid_search()...
  â€¢ Collection: rag_messages
  â€¢ Query text: "How do I write memory-safe high-performance database code?"
  â€¢ Vector dimensions: 1536
  â€¢ Limit: 5 results
  â€¢ Search type: Semantic (vector) + Keyword (text)
  â€¢ Server combines both scores for relevance ranking
  âœ“ Search completed in 0.091s

âœ“ Found 5 related messages across all conversations:
  1. [Score: 0.000] From conv_database_design
     Use NoSQL when you need: flexible schemas, horizontal scaling, high write throughput, or when working with unstructured data. SQL is better for complex queries, ACID transactions, and structured data with well-defined relationships.

  2. [Score: 0.000] From conv_database_design
     What is database normalization?

  3. [Score: 0.000] From conv_database_design
     Database normalization is the process of organizing data to reduce redundancy and improve data integrity. It involves dividing large tables into smaller ones and defining relationships between them using foreign keys.

  4. [Score: 0.000] From conv_database_design
     When should I use NoSQL over SQL?

  5. [Score: 0.000] From conv_performance
     How can I optimize database queries?

=== Step 4: Generating Context-Aware Response ===
âœ“ AI Response (with context from 3 conversations):

Writing memory-safe and high-performance database code involves several best practices and guidelines. Here are some:

1. Use the appropriate database: As discussed in Context 1 and 4, choosing the right database for your needs is crucial. If you need to deal with unstructured data, high write throughput, or horizontal scaling, NoSQL databases are typically a better choice. For structured data, complex queries, and ACID transactions, SQL databases are more suitable.

2. Normalize your data: As mentioned in Context 2 and 3, database normalization can help improve data integrity and reduce redundancy. This can lead to more efficient use of memory and better overall performance.

3. Indexing: Use indexing wisely to speed up your queries. But remember, while indexes speed up retrieval times, they slow down write times because the index also needs to be updated. So it's a trade-off which must be managed.

4. Use prepared statements: Prepared statements can be compiled and cached by the database management system (DBMS), which can lead to performance improvements. They also help prevent SQL injection attacks, which can lead to memory safety issues.

5. Limit your data: Do not retrieve more data than necessary. Use LIMIT in SQL, or its equivalent in NoSQL databases, to avoid loading more data into memory than you need.

6. Use efficient queries: As asked in Context 5, optimizing your database queries is crucial for performance. Avoid using SELECT *, use JOIN instead of nested queries, and make use of aggregate functions for calculations instead of retrieving and processing data in your code.

7. Handle memory explicitly: If using a language like C++, manage memory allocation/deallocation explicitly and use smart pointers to avoid memory leaks.

8. Use proper error handling: Ensure that failed database calls do not lead to memory leaks by implementing proper error handling.

9. Connection Pooling: Repeatedly opening and closing database connections can lead to a significant performance loss. Connection pooling allows you to reuse existing connections, reducing the overhead of establishing a new connection for each query.

Remember, the best practices can vary depending on the specific DBMS and programming language you are using. Always refer to their specific documentation for more in-depth guidelines and best practices.

=== Step 5: Storing New Conversation ===
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 58 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.389s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 2354 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.317s
    â€¢ Function auto-cleaned up by client
âœ“ New conversation stored and indexed for future retrieval

=== Step 6: Cross-Conversation Search ===
Searching for messages about 'ownership' across ALL conversations...

âœ“ Found 3 messages mentioning ownership:
  1. From conv_performance: Rust's ownership system provides zero-cost memory management. Use Box for heap allocation, Rc/Arc for shared ownership, and avoid cloning large data structures. The compiler optimizes away unnecessary allocations.

  2. From conv_rust_programming: Rust's key features include: memory safety without garbage collection, zero-cost abstractions, ownership system, powerful type system, and excellent concurrency support.

  3. From conv_rust_programming: The borrow checker enforces Rust's ownership rules at compile time. It ensures that references don't outlive the data they point to and prevents data races by allowing either multiple immutable references or one mutable reference.

=== System Statistics ===
Total conversations: 4
Total messages stored: 14
All messages are indexed for vector search âœ“
All messages are indexed for text search âœ“
All messages are queryable by metadata âœ“

=== Step 8: Dynamic Search Configuration ===
Each conversation can have its own search config...

ğŸ’¡ Conversations can store custom search configurations:
  â€¢ Search type: hybrid, text, or vector
  â€¢ Relevance thresholds
  â€¢ Filter by tags or metadata
  â€¢ Collection-specific settings
  â€¢ Per-conversation AI behavior

This enables context-aware search tuned to each conversation's needs!

=== Cleanup ===
âœ“ Cleanup complete


=== ğŸ“š Summary: What This Example Showed ===

ğŸ”§ ekoDB Native Capabilities Used:
  âœ“ Functions with Embed operation (AI integration)
  âœ“ Hybrid Search (text + vector combined)
  âœ“ Text Search (full-text with stemming)
  âœ“ Automatic embedding generation
  âœ“ Cross-collection queries

ğŸš€ New Client Helper Methods:
  â€¢ client.embed(text, model) - Generate embeddings
  â€¢ client.hybrid_search() - Semantic + keyword search
  â€¢ client.text_search() - Full-text search
  â€¢ client.find_all() - Query all documents

ğŸ’¡ Key Takeaways:
  1. ekoDB handles AI Functions natively - no external services needed
  2. One-line embedding generation with auto-cleanup
  3. Hybrid search combines semantic understanding + keyword matching
  4. Perfect for RAG: store, search, and retrieve context
  5. All AI capabilities accessible through simple client methods

ğŸ¯ Build production RAG systems with ekoDB!
   â†’ Set OPENAI_API_KEY in your ekoDB server environment
   â†’ Use these client helpers to make AI integration simple
   â†’ Scale to millions of documents with native indexing


[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m
[36mRunning TypeScript RAG Example...[0m
[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m
=== ekoDB RAG Conversation System ===

This example shows how ekoDB can power a self-improving AI system
that learns from its own conversation history.

=== Step 1: Building Conversation History ===
Storing previous conversations with embeddings...

  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 34 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.485s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 169 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.389s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 33 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.429s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 230 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.241s
    â€¢ Function auto-cleaned up by client
âœ“ Stored Rust programming conversation (4 messages)
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 31 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.513s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 217 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.314s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 33 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.296s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 232 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.399s
    â€¢ Function auto-cleaned up by client
âœ“ Stored database design conversation (4 messages)
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 36 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.806s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 178 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.208s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 37 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.388s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 213 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.390s
    â€¢ Function auto-cleaned up by client
âœ“ Stored performance optimization conversation (4 messages)

=== Step 2: New User Question with Context Retrieval ===
User asks: "How do I write memory-safe high-performance database code?"

=== Step 3: Searching Related Context ===
Using hybrid search to find relevant messages from all conversations...


â†’ Generating embedding for user question...
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 58 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.415s
    â€¢ Function auto-cleaned up by client

â†’ Executing hybridSearch()...
  â€¢ Collection: rag_messages
  â€¢ Query text: "How do I write memory-safe high-performance database code?"
  â€¢ Vector dimensions: 1536
  â€¢ Limit: 5 results
  â€¢ Search type: Semantic (vector) + Keyword (text)
  â€¢ Server combines both scores for relevance ranking
  âœ“ Search completed in 0.087s
âœ“ Found 5 related messages across all conversations:
  1. [Score: 0.000] From conv_performance
     How can I optimize database queries?

  2. [Score: 0.000] From conv_database_design
     Use NoSQL when you need: flexible schemas, horizontal scaling, high write throughput, or when working with unstructured data. SQL is better for complex queries, ACID transactions, and structured data with well-defined relationships.

  3. [Score: 0.000] From conv_database_design
     When should I use NoSQL over SQL?

  4. [Score: 0.000] From conv_database_design
     What is database normalization?

  5. [Score: 0.000] From conv_database_design
     Database normalization is the process of organizing data to reduce redundancy and improve data integrity. It involves dividing large tables into smaller ones and defining relationships between them using foreign keys.

=== Step 4: Generating Context-Aware Response ===
âœ“ AI Response (with context from 3 conversations):

Writing memory-safe, high-performance database code requires an understanding of both coding best practices and database architecture. Here are some tips based on the context given:

1. Understand the Database System: You need to understand whether you are using a SQL or NoSQL database, as the design considerations and optimization techniques can differ. SQL is better for complex queries, ACID transactions, and structured data with well-defined relationships. In contrast, NoSQL is more suitable for flexible schemas, horizontal scaling, high write throughput, and unstructured data.

2. Optimize Your Queries: You can optimize database queries by using indexation, pagination, and limiting the fields returned by the queries. Avoid using "SELECT *" as it can slow down your database by returning unnecessary data. Also, make use of JOINs instead of multiple queries when you need data from multiple tables.

3. Normalize Your Database: Normalization is a process that organizes data to minimize redundancy and improve data integrity. It can help enhance the performance of the database by reducing the amount of data that needs to be read from the disk.

4. Memory Management: When writing code, ensure you're effectively managing memory. Avoid memory leaks by properly deallocating any memory that was previously allocated when it is no longer needed. In languages like C++, this is a manual process, but in languages like Java or Python, this is handled by the Garbage Collector.

5. Use Prepared Statements: Prepared statements not only provide protection against SQL injection but can also improve performance as they are parsed only once but can be executed multiple times.

6. Connection Pooling: Connection pooling can significantly reduce the overhead of creating a new connection every time a database operation is performed. By reusing database connections, you can improve the performance of your application.

7. Caching: Implement a caching strategy to store and retrieve frequently accessed data, which can help increase your application's performance and responsiveness.

Remember, writing high-performance database code is an iterative process and involves regular monitoring and tuning based on the specific needs of your application.

=== Step 5: Storing New Conversation ===
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 58 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.458s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 2256 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.257s
    â€¢ Function auto-cleaned up by client
âœ“ New conversation stored and indexed for future retrieval

=== Step 6: Cross-Conversation Search ===
Searching for messages about 'ownership' across ALL conversations...


â†’ Executing textSearch()...
  â€¢ Collection: rag_messages
  â€¢ Query: "ownership system"
  â€¢ Limit: 3 results
  â€¢ Search method: Full-text with fuzzy matching & stemming
  â€¢ No vector embeddings needed - pure keyword search
  âœ“ Text search completed in 0.051s
âœ“ Found 3 messages mentioning ownership:
  1. From conv_performance: Rust's ownership system provides zero-cost memory management. Use Box for heap allocation, Rc/Arc for shared ownership, and avoid cloning large data structures. The compiler optimizes away unnecessary allocations.

  2. From conv_rust_programming: Rust's key features include: memory safety without garbage collection, zero-cost abstractions, ownership system, powerful type system, and excellent concurrency support.

  3. From conv_rust_programming: The borrow checker enforces Rust's ownership rules at compile time. It ensures that references don't outlive the data they point to and prevents data races by allowing either multiple immutable references or one mutable reference.

=== System Statistics ===

â†’ Querying database statistics...
  â€¢ Using findAllWithLimit() helper - simplified query API

ğŸ“Š Database Statistics:
  â€¢ Total conversations: 4
  â€¢ Total messages stored: 14
  â€¢ All messages indexed for vector search âœ“
  â€¢ All messages indexed for text search âœ“
  â€¢ All messages queryable by metadata âœ“

=== Step 8: Dynamic Search Configuration ===
Each conversation can have its own search config...

ğŸ’¡ Conversations can store custom search configurations:
  â€¢ Search type: hybrid, text, or vector
  â€¢ Relevance thresholds
  â€¢ Filter by tags or metadata
  â€¢ Collection-specific settings
  â€¢ Per-conversation AI behavior

This enables context-aware search tuned to each conversation's needs!

=== Cleanup ===
âœ“ Cleanup complete


=== ğŸ“š Summary: What This Example Showed ===

ğŸ”§ ekoDB Native Capabilities Used:
  âœ“ Functions with Embed operation (AI integration)
  âœ“ Hybrid Search (text + vector combined)
  âœ“ Text Search (full-text with stemming)
  âœ“ Automatic embedding generation
  âœ“ Cross-collection queries

ğŸš€ New Client Helper Methods:
  â€¢ client.embed(text, model) - Generate embeddings
  â€¢ client.hybridSearch() - Semantic + keyword search
  â€¢ client.textSearch() - Full-text search
  â€¢ client.findAllWithLimit() - Query all documents

ğŸ’¡ Key Takeaways:
  1. ekoDB handles AI Functions natively - no external services needed
  2. One-line embedding generation with auto-cleanup
  3. Hybrid search combines semantic understanding + keyword matching
  4. Perfect for RAG: store, search, and retrieve context
  5. All AI capabilities accessible through simple client methods

ğŸ¯ Build production RAG systems with ekoDB!
   â†’ Set OPENAI_API_KEY in your ekoDB server environment
   â†’ Use these client helpers to make AI integration simple
   â†’ Scale to millions of documents with native indexing


[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m
[36mRunning Go RAG Example...[0m
[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m
=== ekoDB RAG Conversation System ===

This example shows how ekoDB can power a self-improving AI system
that learns from its own conversation history.

=== Step 1: Building Conversation History ===
Storing previous conversations with embeddings...

  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 34 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.289s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 169 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.211s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 33 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.382s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 230 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.249s
    â€¢ Function auto-cleaned up by client
âœ“ Stored Rust programming conversation (4 messages)
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 31 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.654s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 217 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.286s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 33 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.513s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 232 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.381s
    â€¢ Function auto-cleaned up by client
âœ“ Stored database design conversation (4 messages)
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 36 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.431s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 178 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.224s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 37 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.402s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 213 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.254s
    â€¢ Function auto-cleaned up by client
âœ“ Stored performance optimization conversation (4 messages)

=== Step 2: New User Question with Context Retrieval ===
User asks: "How do I write memory-safe high-performance database code?"

=== Step 3: Searching Related Context ===
Using hybrid search to find relevant messages from all conversations...


â†’ Generating embedding for user question...
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 58 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.407s
    â€¢ Function auto-cleaned up by client

â†’ Executing HybridSearch()...
  â€¢ Collection: rag_messages
  â€¢ Query text: "How do I write memory-safe high-performance database code?"
  â€¢ Vector dimensions: 1536
  â€¢ Limit: 5 results
  â€¢ Search type: Semantic (vector) + Keyword (text)
  â€¢ Server combines both scores for relevance ranking
  âœ“ Search completed in 0.089s

âœ“ Found 5 related messages across all conversations:
  1. [Score: 0.000] From conv_performance
     How can I optimize database queries?

  2. [Score: 0.000] From conv_database_design
     Use NoSQL when you need: flexible schemas, horizontal scaling, high write throughput, or when working with unstructured data. SQL is better for complex queries, ACID transactions, and structured data with well-defined relationships.

  3. [Score: 0.000] From conv_database_design
     When should I use NoSQL over SQL?

  4. [Score: 0.000] From conv_database_design
     What is database normalization?

  5. [Score: 0.000] From conv_database_design
     Database normalization is the process of organizing data to reduce redundancy and improve data integrity. It involves dividing large tables into smaller ones and defining relationships between them using foreign keys.

=== Step 4: Generating Context-Aware Response ===
âœ“ AI Response (with context from 3 conversations):

Writing memory-safe, high-performance database code involves several steps and techniques. Here are some of them:

1. **Optimize Database Queries**: This is an important step for improving the performance of your database code. This can be done through proper indexing, limiting the data that you are querying with more specific WHERE clauses, or using JOINs instead of multiple queries. Also, take advantage of stored procedures, views, and triggers where appropriate.

2. **Database Normalization**: This process helps to reduce data redundancy and improve data integrity, which indirectly contributes to performance enhancement. By organizing data into smaller tables and defining relationships through foreign keys, you can speed up queries and make your database more efficient.

3. **Choose the Right Database Type**: Depending on your needs, either SQL or NoSQL may be more appropriate. If you need flexible schemas, high write throughput, horizontal scaling, or are working with unstructured data, NoSQL might be a better choice. On the other hand, if you require complex queries, ACID transactions, or have structured data with well-defined relationships, SQL is likely the better option.

4. **Memory Management**: Be mindful of the data types you use, as this directly affects memory usage. Use appropriate data types that take up less memory but still satisfy your needs. Also, be careful with how much data you load into memory at once. Try to process data in chunks rather than loading large datasets into memory all at once.

5. **Error Handling and Security**: Properly handle database errors and exceptions to prevent memory leaks and ensure the stability of your application. Also, use parameterized queries or prepared statements to avoid SQL injection attacks, which can compromise your database.

6. **Use Caching**: Caching can help to reduce the load on your database and make your application faster. By storing the results of common queries in memory, you can avoid having to execute those queries repeatedly.

By following these practices, you can write memory-safe and high-performance database code.

=== Step 5: Storing New Conversation ===
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 58 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.339s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 2127 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.818s
    â€¢ Function auto-cleaned up by client
âœ“ New conversation stored and indexed for future retrieval

=== Step 6: Cross-Conversation Search ===
Searching for messages about 'ownership' across ALL conversations...


â†’ Executing TextSearch()...
  â€¢ Collection: rag_messages
  â€¢ Query: "ownership system"
  â€¢ Limit: 3 results
  â€¢ Search method: Full-text with fuzzy matching & stemming
  â€¢ No vector embeddings needed - pure keyword search
  âœ“ Text search completed in 0.053s

âœ“ Found 3 messages mentioning ownership:
  1. From conv_rust_programming: Rust's key features include: memory safety without garbage collection, zero-cost abstractions, ownership system, powerful type system, and excellent concurrency support.

  2. From conv_performance: Rust's ownership system provides zero-cost memory management. Use Box for heap allocation, Rc/Arc for shared ownership, and avoid cloning large data structures. The compiler optimizes away unnecessary allocations.

  3. From conv_rust_programming: The borrow checker enforces Rust's ownership rules at compile time. It ensures that references don't outlive the data they point to and prevents data races by allowing either multiple immutable references or one mutable reference.

=== System Statistics ===

â†’ Querying database statistics...
  â€¢ Using FindAll() helper - simplified query API

ğŸ“Š Database Statistics:
  â€¢ Total conversations: 4
  â€¢ Total messages stored: 14
  â€¢ All messages indexed for vector search âœ“
  â€¢ All messages indexed for text search âœ“
  â€¢ All messages queryable by metadata âœ“

=== Step 8: Dynamic Search Configuration ===
Each conversation can have its own search config...

ğŸ’¡ Conversations can store custom search configurations:
  â€¢ Search type: hybrid, text, or vector
  â€¢ Relevance thresholds
  â€¢ Filter by tags or metadata
  â€¢ Collection-specific settings
  â€¢ Per-conversation AI behavior

This enables context-aware search tuned to each conversation's needs!

=== Cleanup ===
âœ“ Cleanup complete


=== ğŸ“š Summary: What This Example Showed ===

ğŸ”§ ekoDB Native Capabilities Used:
  âœ“ Functions with Embed operation (AI integration)
  âœ“ Hybrid Search (text + vector combined)
  âœ“ Text Search (full-text with stemming)
  âœ“ Automatic embedding generation
  âœ“ Cross-collection queries

ğŸš€ New Client Helper Methods:
  â€¢ client.Embed(text, model) - Generate embeddings
  â€¢ client.HybridSearch() - Semantic + keyword search
  â€¢ client.TextSearch() - Full-text search
  â€¢ client.FindAll() - Query all documents

ğŸ’¡ Key Takeaways:
  1. ekoDB handles AI Functions natively - no external services needed
  2. One-line embedding generation with auto-cleanup
  3. Hybrid search combines semantic understanding + keyword matching
  4. Perfect for RAG: store, search, and retrieve context
  5. All AI capabilities accessible through simple client methods

ğŸ¯ Build production RAG systems with ekoDB!
   â†’ Set OPENAI_API_KEY in your ekoDB server environment
   â†’ Use these client helpers to make AI integration simple
   â†’ Scale to millions of documents with native indexing


[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m
[36mRunning Kotlin RAG Example...[0m
[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m
=== ekoDB RAG Conversation System ===

This example shows how ekoDB can power a self-improving AI system
that learns from its own conversation history.

SLF4J(W): No SLF4J providers were found.
SLF4J(W): Defaulting to no-operation (NOP) logger implementation
SLF4J(W): See https://www.slf4j.org/codes.html#noProviders for further details.
=== Step 1: Building Conversation History ===
Storing previous conversations with embeddings...

  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 34 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.383s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 169 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.578s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 33 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.294s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 230 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.35s
    â€¢ Function auto-cleaned up by client
âœ“ Stored Rust programming conversation (4 messages)
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 31 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.356s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 217 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.608s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 33 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.454s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 232 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.501s
    â€¢ Function auto-cleaned up by client
âœ“ Stored database design conversation (4 messages)
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 36 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.238s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 178 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.284s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 37 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.332s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 213 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.284s
    â€¢ Function auto-cleaned up by client
âœ“ Stored performance optimization conversation (4 messages)

=== Step 2: New User Question with Context Retrieval ===
User asks: "How do I write memory-safe high-performance database code?"

=== Step 3: Searching Related Context ===
Using hybrid search to find relevant messages from all conversations...


â†’ Generating embedding for user question...
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 58 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.271s
    â€¢ Function auto-cleaned up by client

â†’ Executing hybridSearch()...
  â€¢ Collection: rag_messages
  â€¢ Query text: "How do I write memory-safe high-performance database code?"
  â€¢ Vector dimensions: 1536
  â€¢ Limit: 5 results
  â€¢ Search type: Semantic (vector) + Keyword (text)
  â€¢ Server combines both scores for relevance ranking
  âœ“ Search completed in 0.083s

âœ“ Found 5 related messages across all conversations:
  1. [Score: 0.000] From conv_performance
     How can I optimize database queries?

  2. [Score: 0.000] From conv_database_design
     Use NoSQL when you need: flexible schemas, horizontal scaling, high write throughput, or when working with unstructured data. SQL is better for complex queries, ACID transactions, and structured data with well-defined relationships.

  3. [Score: 0.000] From conv_database_design
     When should I use NoSQL over SQL?

  4. [Score: 0.000] From conv_database_design
     What is database normalization?

  5. [Score: 0.000] From conv_database_design
     Database normalization is the process of organizing data to reduce redundancy and improve data integrity. It involves dividing large tables into smaller ones and defining relationships between them using foreign keys.

=== Step 4: Generating Context-Aware Response ===
âœ“ Context prepared from search results
âœ“ AI would use this context to generate comprehensive response

=== Step 5: Storing New Conversation ===
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 58 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.704s
    â€¢ Function auto-cleaned up by client
âœ“ New conversation stored and indexed for future retrieval

=== Step 6: Cross-Conversation Search ===
Searching for messages about 'ownership' across ALL conversations...


â†’ Executing textSearch()...
  â€¢ Collection: rag_messages
  â€¢ Query: "ownership system"
  â€¢ Limit: 3 results
  â€¢ Search method: Full-text with fuzzy matching & stemming
  â€¢ No vector embeddings needed - pure keyword search
  âœ“ Text search completed in 0.047s

âœ“ Found 3 messages mentioning ownership:
  1. From conv_rust_programming: Rust's key features include: memory safety without garbage collection, zero-cost abstractions, ownership system, powerful type system, and excellent concurrency support.

  2. From conv_performance: Rust's ownership system provides zero-cost memory management. Use Box for heap allocation, Rc/Arc for shared ownership, and avoid cloning large data structures. The compiler optimizes away unnecessary allocations.

  3. From conv_rust_programming: The borrow checker enforces Rust's ownership rules at compile time. It ensures that references don't outlive the data they point to and prevents data races by allowing either multiple immutable references or one mutable reference.

=== System Statistics ===

â†’ Querying database statistics...
  â€¢ Using findAllWithLimit() helper - simplified query API

ğŸ“Š Database Statistics:
  â€¢ Total conversations: 4
  â€¢ Total messages stored: 13
  â€¢ All messages indexed for vector search âœ“
  â€¢ All messages indexed for text search âœ“
  â€¢ All messages queryable by metadata âœ“

=== Step 8: Dynamic Search Configuration ===
Each conversation can have its own search config...

ğŸ’¡ Conversations can store custom search configurations:
  â€¢ Search type: hybrid, text, or vector
  â€¢ Relevance thresholds
  â€¢ Filter by tags or metadata
  â€¢ Collection-specific settings
  â€¢ Per-conversation AI behavior

This enables context-aware search tuned to each conversation's needs!

=== Cleanup ===
âœ“ Cleanup complete


=== ğŸ“š Summary: What This Example Showed ===

ğŸ”§ ekoDB Native Capabilities Used:
  âœ“ Functions with Embed operation (AI integration)
  âœ“ Hybrid Search (text + vector combined)
  âœ“ Text Search (full-text with stemming)
  âœ“ Automatic embedding generation
  âœ“ Cross-collection queries

ğŸš€ New Client Helper Methods:
  â€¢ client.embed(text, model) - Generate embeddings
  â€¢ client.hybridSearch() - Semantic + keyword search
  â€¢ client.textSearch() - Full-text search
  â€¢ client.findAllWithLimit() - Query all documents

ğŸ’¡ Key Takeaways:
  1. ekoDB handles AI Functions natively - no external services needed
  2. One-line embedding generation with auto-cleanup
  3. Hybrid search combines semantic understanding + keyword matching
  4. Perfect for RAG: store, search, and retrieve context
  5. All AI capabilities accessible through simple client methods

ğŸ¯ Build production RAG systems with ekoDB!
   â†’ Set OPENAI_API_KEY in your ekoDB server environment
   â†’ Use these client helpers to make AI integration simple
   â†’ Scale to millions of documents with native indexing


[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m
âœ… [32mRAG Examples Complete![0m
[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m

[32mWhat you just saw across 5 languages:[0m
  âœ“ Embeddings generated via ekoDB Functions
  âœ“ Hybrid search (semantic + keyword)
  âœ“ Text search with stemming
  âœ“ Cross-conversation context retrieval
  âœ“ Simple client helpers wrapping powerful AI

[36mMission: AI for All ğŸš€[0m - Making RAG accessible to everyone!

