make test-examples-rag

ğŸ¤– [36mRAG Conversation System Examples[0m
[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m

[33mPrerequisites:[0m
  1. ekoDB server running (make run in ekodb/)
  2. OPENAI_API_KEY set in server environment
  3. API_BASE_URL and API_BASE_KEY exported in your shell

[36mBuilding Rust client library...[0m
âœ“ Rust client built

[36mBuilding Python client bindings...[0m
ğŸ¹ Building a mixed python/rust project
ğŸ”— Found pyo3 bindings with abi3 support
ğŸ Not using a specific python interpreter
ğŸ’» Using `MACOSX_DEPLOYMENT_TARGET=11.0` for aarch64-apple-darwin by default
ğŸ“¦ Built wheel for abi3 Python â‰¥ 3.8 to /Users/tek/Development/ekoDB/ekodb-client/ekodb-client-py/target/wheels/ekodb_client-0.10.0-cp38-abi3-macosx_11_0_arm64.whl
âœ“ Python client built and installed

[36mBuilding TypeScript client library...[0m
âœ“ TypeScript client built

[36mInstalling TypeScript client in examples...[0m
âœ“ TypeScript client installed

[36mBuilding TypeScript example...[0m
âœ“ TypeScript example built

[36mBuilding Go client library...[0m
âœ“ Go client built

[36mBuilding Go RAG example...[0m
âœ“ Go example built

[36mBuilding Kotlin client library...[0m
âœ“ Kotlin client built

[36mBuilding Kotlin RAG example...[0m
âœ“ Kotlin example built

[36mRunning Rust RAG Example...[0m
[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m
=== ekoDB RAG Conversation System ===

This example shows how ekoDB can power a self-improving AI system
that learns from its own conversation history.

=== Step 1: Building Conversation History ===
Storing previous conversations with embeddings...

  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 34 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 1.319076709s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 169 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 304.975917ms
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 33 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 356.898708ms
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 230 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 1.163721208s
    â€¢ Function auto-cleaned up by client
âœ“ Stored Rust programming conversation (4 messages)
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 31 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 328.361958ms
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 217 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 850.153041ms
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 33 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 288.219958ms
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 232 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 341.785ms
    â€¢ Function auto-cleaned up by client
âœ“ Stored database design conversation (4 messages)
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 36 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 401.508583ms
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 178 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 349.759042ms
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 37 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 324.202333ms
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 213 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 362.522667ms
    â€¢ Function auto-cleaned up by client
âœ“ Stored performance optimization conversation (4 messages)

=== Step 2: New User Question with Context Retrieval ===
User asks: "How do I write memory-safe high-performance database code?"

=== Step 3: Searching Related Context ===
Using hybrid search to find relevant messages from all conversations...


â†’ Generating embedding for user question...
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 58 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 279.605916ms
    â€¢ Function auto-cleaned up by client

â†’ Executing hybrid_search()...
  â€¢ Collection: rag_messages
  â€¢ Query text: "How do I write memory-safe high-performance database code?"
  â€¢ Vector dimensions: 1536
  â€¢ Limit: 5 results
  â€¢ Search type: Semantic (vector) + Keyword (text)
  â€¢ Server combines both scores for relevance ranking
  âœ“ Search completed in 112.86275ms

âœ“ Found 5 related messages across all conversations:
  1. From conv_database_design
     Use NoSQL when you need: flexible schemas, horizontal scaling, high write throughput, or when working with unstructured data. SQL is better for complex queries, ACID transactions, and structured data with well-defined relationships.

  2. From conv_database_design
     What is database normalization?

  3. From conv_database_design
     Database normalization is the process of organizing data to reduce redundancy and improve data integrity. It involves dividing large tables into smaller ones and defining relationships between them using foreign keys.

  4. From conv_database_design
     When should I use NoSQL over SQL?

  5. From conv_performance
     How can I optimize database queries?

=== Step 4: Generating Context-Aware Response ===
âœ“ AI Response (with context from 3 conversations):

Writing memory-safe, high-performance database code involves several key principles. Here are some general tips:

1. Understand Your Data and Queries: The first step to optimizing database code is understanding your data and the queries you will be running. If you're dealing with structured data with well-defined relationships and need to perform complex queries, SQL databases might be a better option. If you need flexible schemas, high write throughput, or are working with unstructured data, NoSQL databases might be a better fit.

2. Normalize Your Database: As mentioned in Context 3, database normalization can help reduce redundancy and improve data integrity. By dividing large tables into smaller ones and defining relationships between them using foreign keys, you can write more efficient and memory-safe code.

3. Optimize Your Queries: To improve the performance of your database code, you should optimize your queries. This could involve using indexes to speed up data retrieval, using batch processing to reduce the number of round trips to the database, limiting the returned data by using SELECT statements wisely, and avoiding using wildcard characters as much as possible.

4. Handle Memory Management: When writing high-performance database code, it's important to manage memory carefully. This includes releasing any resources that are no longer needed and avoiding memory leaks. In managed languages like Java or C#, the garbage collector will generally handle this for you. In unmanaged languages like C or C++, you'll need to manually deallocate memory.

5. Use Connection Pooling: Connection pooling can greatly increase the performance of your database code by reusing existing connections rather than creating a new one every time a query is made. This can reduce the overhead of constantly creating and closing connections.

6. Test and Monitor Performance: Regularly test your database code for performance and monitor it in production to identify any potential bottlenecks or areas for improvement.

Remember, the choice between SQL and NoSQL, as well as how you optimize your queries, will depend on your specific use case and the nature of your data.

=== Step 5: Storing New Conversation ===
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 58 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 300.618625ms
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 2185 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 391.533ms
    â€¢ Function auto-cleaned up by client
âœ“ New conversation stored and indexed for future retrieval

=== Step 6: Cross-Conversation Search ===
Searching for messages about 'ownership' across ALL conversations...


â†’ Executing text_search()...
  â€¢ Collection: rag_messages
  â€¢ Query: "ownership system"
  â€¢ Limit: 3 results
  â€¢ Search method: Full-text with fuzzy matching & stemming
  â€¢ No vector embeddings needed - pure keyword search
  âœ“ Text search completed in 71.593625ms

âœ“ Found 3 messages mentioning ownership:
  1. From conv_performance: Rust's ownership system provides zero-cost memory management. Use Box for heap allocation, Rc/Arc for shared ownership, and avoid cloning large data structures. The compiler optimizes away unnecessary allocations.

  2. From conv_rust_programming: Rust's key features include: memory safety without garbage collection, zero-cost abstractions, ownership system, powerful type system, and excellent concurrency support.

  3. From conv_rust_programming: The borrow checker enforces Rust's ownership rules at compile time. It ensures that references don't outlive the data they point to and prevents data races by allowing either multiple immutable references or one mutable reference.

=== System Statistics ===

â†’ Querying database statistics...
  â€¢ Using find_all() helper - simplified query API

ğŸ“Š Database Statistics:
  â€¢ Total conversations: 4
  â€¢ Total messages stored: 14
  â€¢ All messages indexed for vector search âœ“
  â€¢ All messages indexed for text search âœ“
  â€¢ All messages queryable by metadata âœ“

=== Step 8: Dynamic Search Configuration ===
Each conversation can have its own search config...

ğŸ’¡ Conversations can store custom search configurations:
  â€¢ Search type: hybrid, text, or vector
  â€¢ Relevance thresholds
  â€¢ Filter by tags or metadata
  â€¢ Collection-specific settings
  â€¢ Per-conversation AI behavior

This enables context-aware search tuned to each conversation's needs!


=== Cleanup ===
Deleting example collections...

âœ… All done! RAG system demonstrated successfully.

âœ“ Using search results to enhance AI responses (RAG)
âœ“ Building a self-improving knowledge base
âœ“ Dynamic search configurations per conversation

ekoDB provides everything needed for AI-powered applications:
  â€¢ Vector search (semantic similarity)
  â€¢ Text search (keyword matching)
  â€¢ Hybrid search (best of both)
  â€¢ AI functions (Chat, Embed)
  â€¢ Flexible querying and filtering
  â€¢ All in one database - no external dependencies!


[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m
[36mRunning Python RAG Example...[0m
[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m
=== ekoDB RAG Conversation System ===

This example shows how ekoDB can power a self-improving AI system
that learns from its own conversation history.

=== Step 1: Building Conversation History ===
Storing previous conversations with embeddings...

  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 34 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 1.023s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 169 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.322s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 33 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.339s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 230 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.284s
    â€¢ Function auto-cleaned up by client
âœ“ Stored Rust programming conversation (4 messages)
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 31 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.222s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 217 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.250s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 33 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 1.145s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 232 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.483s
    â€¢ Function auto-cleaned up by client
âœ“ Stored database design conversation (4 messages)
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 36 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.500s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 178 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.360s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 37 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.426s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 213 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 1.007s
    â€¢ Function auto-cleaned up by client
âœ“ Stored performance optimization conversation (4 messages)

=== Step 2: New User Question with Context Retrieval ===
User asks: "How do I write memory-safe high-performance database code?"

=== Step 3: Searching Related Context ===
Using hybrid search to find relevant messages from all conversations...


â†’ Generating embedding for user question...
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 58 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.367s
    â€¢ Function auto-cleaned up by client

â†’ Executing hybrid_search()...
  â€¢ Collection: rag_messages
  â€¢ Query text: "How do I write memory-safe high-performance database code?"
  â€¢ Vector dimensions: 1536
  â€¢ Limit: 5 results
  â€¢ Search type: Semantic (vector) + Keyword (text)
  â€¢ Server combines both scores for relevance ranking
  âœ“ Search completed in 0.103s

âœ“ Found 5 related messages across all conversations:
  1. [Score: 0.000] From conv_database_design
     Use NoSQL when you need: flexible schemas, horizontal scaling, high write throughput, or when working with unstructured data. SQL is better for complex queries, ACID transactions, and structured data with well-defined relationships.

  2. [Score: 0.000] From conv_database_design
     What is database normalization?

  3. [Score: 0.000] From conv_database_design
     Database normalization is the process of organizing data to reduce redundancy and improve data integrity. It involves dividing large tables into smaller ones and defining relationships between them using foreign keys.

  4. [Score: 0.000] From conv_database_design
     When should I use NoSQL over SQL?

  5. [Score: 0.000] From conv_performance
     How can I optimize database queries?

=== Step 4: Generating Context-Aware Response ===
âœ“ AI Response (with context from 3 conversations):

Writing memory-safe, high-performance database code involves several aspects, including but not limited to:

1. **Normalization**: Normalizing your database as per Context 3 can help to reduce redundancy and improve data integrity. It also makes the database more efficient and easier to work with.

2. **Appropriate Use of SQL or NoSQL**: Depending on your specific needs, you may choose to use SQL or NoSQL. As mentioned in Context 1 and 4, SQL is better for complex queries, ACID transactions and structured data with well-defined relationships. NoSQL is better when you need flexible schemas, high write throughput, horizontal scaling or are working with unstructured data.

3. **Indexing**: Indexing is a technique to optimize the performance of a database by minimizing the number of disk accesses required when a query is processed. It is a data structure technique which is used to quickly locate and access the data in a database.

4. **Query Optimization**: There are several ways you can optimize your queries. You should avoid using SELECT *, avoid unnecessary joins, and use WHERE clause instead of HAVING where possible. Also, avoid NULL checks since they can't use indices. 

5. **Memory Management**: In terms of safety, you should always ensure that any memory that has been allocated is properly deallocated when it is no longer needed. This helps to prevent memory leaks, which can lead to decreased performance and, in severe cases, cause the application to crash.

6. **Concurrency Control**: Implementing proper concurrency control can help to prevent conflicts and ensure that transactions are executed atomically. This can be particularly important in multi-user database systems.

7. **Caching**: Database caching can greatly enhance the performance of your database code. By storing frequently accessed data in memory, you can reduce the need for disk accesses, which are often a bottleneck in database systems.

8. **Use Prepared Statements**: Prepared statements can improve performance as they're only parsed once but can be executed many times. They also help mitigate SQL injection attacks, thus improving security.

Remember, the best practices vary based on the specifics of the project, the database system being used, and the problem constraints.

=== Step 5: Storing New Conversation ===
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 58 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.415s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 2281 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.237s
    â€¢ Function auto-cleaned up by client
âœ“ New conversation stored and indexed for future retrieval

=== Step 6: Cross-Conversation Search ===
Searching for messages about 'ownership' across ALL conversations...

âœ“ Found 3 messages mentioning ownership:
  1. From conv_performance: Rust's ownership system provides zero-cost memory management. Use Box for heap allocation, Rc/Arc for shared ownership, and avoid cloning large data structures. The compiler optimizes away unnecessary allocations.

  2. From conv_rust_programming: Rust's key features include: memory safety without garbage collection, zero-cost abstractions, ownership system, powerful type system, and excellent concurrency support.

  3. From conv_rust_programming: The borrow checker enforces Rust's ownership rules at compile time. It ensures that references don't outlive the data they point to and prevents data races by allowing either multiple immutable references or one mutable reference.

=== System Statistics ===
Total conversations: 4
Total messages stored: 14
All messages are indexed for vector search âœ“
All messages are indexed for text search âœ“
All messages are queryable by metadata âœ“

=== Step 8: Dynamic Search Configuration ===
Each conversation can have its own search config...

ğŸ’¡ Conversations can store custom search configurations:
  â€¢ Search type: hybrid, text, or vector
  â€¢ Relevance thresholds
  â€¢ Filter by tags or metadata
  â€¢ Collection-specific settings
  â€¢ Per-conversation AI behavior

This enables context-aware search tuned to each conversation's needs!

=== Cleanup ===
âœ“ Cleanup complete


=== ğŸ“š Summary: What This Example Showed ===

ğŸ”§ ekoDB Native Capabilities Used:
  âœ“ Functions with Embed operation (AI integration)
  âœ“ Hybrid Search (text + vector combined)
  âœ“ Text Search (full-text with stemming)
  âœ“ Automatic embedding generation
  âœ“ Cross-collection queries

ğŸš€ New Client Helper Methods:
  â€¢ client.embed(text, model) - Generate embeddings
  â€¢ client.hybrid_search() - Semantic + keyword search
  â€¢ client.text_search() - Full-text search
  â€¢ client.find_all() - Query all documents

ğŸ’¡ Key Takeaways:
  1. ekoDB handles AI Functions natively - no external services needed
  2. One-line embedding generation with auto-cleanup
  3. Hybrid search combines semantic understanding + keyword matching
  4. Perfect for RAG: store, search, and retrieve context
  5. All AI capabilities accessible through simple client methods

ğŸ¯ Build production RAG systems with ekoDB!
   â†’ Set OPENAI_API_KEY in your ekoDB server environment
   â†’ Use these client helpers to make AI integration simple
   â†’ Scale to millions of documents with native indexing


[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m
[36mRunning TypeScript RAG Example...[0m
[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m
=== ekoDB RAG Conversation System ===

This example shows how ekoDB can power a self-improving AI system
that learns from its own conversation history.

=== Step 1: Building Conversation History ===
Storing previous conversations with embeddings...

  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 34 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.326s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 169 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.241s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 33 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.374s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 230 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.343s
    â€¢ Function auto-cleaned up by client
âœ“ Stored Rust programming conversation (4 messages)
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 31 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.631s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 217 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.298s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 33 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.328s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 232 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.329s
    â€¢ Function auto-cleaned up by client
âœ“ Stored database design conversation (4 messages)
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 36 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.417s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 178 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.300s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 37 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.343s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 213 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.330s
    â€¢ Function auto-cleaned up by client
âœ“ Stored performance optimization conversation (4 messages)

=== Step 2: New User Question with Context Retrieval ===
User asks: "How do I write memory-safe high-performance database code?"

=== Step 3: Searching Related Context ===
Using hybrid search to find relevant messages from all conversations...


â†’ Generating embedding for user question...
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 58 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.316s
    â€¢ Function auto-cleaned up by client

â†’ Executing hybridSearch()...
  â€¢ Collection: rag_messages
  â€¢ Query text: "How do I write memory-safe high-performance database code?"
  â€¢ Vector dimensions: 1536
  â€¢ Limit: 5 results
  â€¢ Search type: Semantic (vector) + Keyword (text)
  â€¢ Server combines both scores for relevance ranking
  âœ“ Search completed in 0.093s
âœ“ Found 5 related messages across all conversations:
  1. [Score: 0.000] From conv_database_design
     Database normalization is the process of organizing data to reduce redundancy and improve data integrity. It involves dividing large tables into smaller ones and defining relationships between them using foreign keys.

  2. [Score: 0.000] From conv_database_design
     What is database normalization?

  3. [Score: 0.000] From conv_database_design
     Use NoSQL when you need: flexible schemas, horizontal scaling, high write throughput, or when working with unstructured data. SQL is better for complex queries, ACID transactions, and structured data with well-defined relationships.

  4. [Score: 0.000] From conv_database_design
     When should I use NoSQL over SQL?

  5. [Score: 0.000] From conv_performance
     How can I optimize database queries?

=== Step 4: Generating Context-Aware Response ===
âœ“ AI Response (with context from 3 conversations):

Writing memory-safe, high-performance database code can be a complex task and involves proper knowledge of the database system you are working with, be it SQL or NoSQL, and efficient programming practices. Here are some general tips:

1. Understand your database: Whether you are using SQL or NoSQL, understanding the strengths and weaknesses of your database system will help you to write more efficient code. For SQL, you should understand normalization and how to create efficient relationships using foreign keys. For NoSQL, you should understand how to create flexible schemas and handle unstructured data.

2. Optimize your queries: An inefficient query can slow down your application and consume more memory. You should learn and use techniques to optimize your queries. This could involve using indexes, reducing the number of joins, only retrieving necessary data, and avoiding complex subqueries when possible.

3. Use appropriate data types: Using the appropriate data types can reduce the memory footprint and improve performance. For example, use integers instead of strings for numerical values.

4. Use ACID transactions: ACID (Atomicity, Consistency, Isolation, Durability) transactions can help to maintain data integrity and prevent data corruption.

5. Avoid N+1 query problem: This is a common problem where you execute one query to get a list of records and then execute an additional query for each record. This can be avoided by using techniques like eager loading.

6. Use connection pooling: Connection pooling can greatly reduce the memory overhead and latency of opening and closing connections for each database interaction.

7. Handle exceptions: Always handle database exceptions to avoid memory leaks and unexpected application behavior.

8. Regularly monitor performance: Use tools to monitor your database's performance. This can help you identify bottlenecks and optimize accordingly.

Remember, the best practices can vary depending on the specific database system you are using and the nature of your application.

=== Step 5: Storing New Conversation ===
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 58 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.379s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 2049 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.332s
    â€¢ Function auto-cleaned up by client
âœ“ New conversation stored and indexed for future retrieval

=== Step 6: Cross-Conversation Search ===
Searching for messages about 'ownership' across ALL conversations...


â†’ Executing textSearch()...
  â€¢ Collection: rag_messages
  â€¢ Query: "ownership system"
  â€¢ Limit: 3 results
  â€¢ Search method: Full-text with fuzzy matching & stemming
  â€¢ No vector embeddings needed - pure keyword search
  âœ“ Text search completed in 0.057s
âœ“ Found 3 messages mentioning ownership:
  1. From conv_rust_programming: Rust's key features include: memory safety without garbage collection, zero-cost abstractions, ownership system, powerful type system, and excellent concurrency support.

  2. From conv_performance: Rust's ownership system provides zero-cost memory management. Use Box for heap allocation, Rc/Arc for shared ownership, and avoid cloning large data structures. The compiler optimizes away unnecessary allocations.

  3. From conv_rust_programming: The borrow checker enforces Rust's ownership rules at compile time. It ensures that references don't outlive the data they point to and prevents data races by allowing either multiple immutable references or one mutable reference.

=== System Statistics ===

â†’ Querying database statistics...
  â€¢ Using findAllWithLimit() helper - simplified query API

ğŸ“Š Database Statistics:
  â€¢ Total conversations: 4
  â€¢ Total messages stored: 14
  â€¢ All messages indexed for vector search âœ“
  â€¢ All messages indexed for text search âœ“
  â€¢ All messages queryable by metadata âœ“

=== Step 8: Dynamic Search Configuration ===
Each conversation can have its own search config...

ğŸ’¡ Conversations can store custom search configurations:
  â€¢ Search type: hybrid, text, or vector
  â€¢ Relevance thresholds
  â€¢ Filter by tags or metadata
  â€¢ Collection-specific settings
  â€¢ Per-conversation AI behavior

This enables context-aware search tuned to each conversation's needs!

=== Cleanup ===
âœ“ Cleanup complete


=== ğŸ“š Summary: What This Example Showed ===

ğŸ”§ ekoDB Native Capabilities Used:
  âœ“ Functions with Embed operation (AI integration)
  âœ“ Hybrid Search (text + vector combined)
  âœ“ Text Search (full-text with stemming)
  âœ“ Automatic embedding generation
  âœ“ Cross-collection queries

ğŸš€ New Client Helper Methods:
  â€¢ client.embed(text, model) - Generate embeddings
  â€¢ client.hybridSearch() - Semantic + keyword search
  â€¢ client.textSearch() - Full-text search
  â€¢ client.findAllWithLimit() - Query all documents

ğŸ’¡ Key Takeaways:
  1. ekoDB handles AI Functions natively - no external services needed
  2. One-line embedding generation with auto-cleanup
  3. Hybrid search combines semantic understanding + keyword matching
  4. Perfect for RAG: store, search, and retrieve context
  5. All AI capabilities accessible through simple client methods

ğŸ¯ Build production RAG systems with ekoDB!
   â†’ Set OPENAI_API_KEY in your ekoDB server environment
   â†’ Use these client helpers to make AI integration simple
   â†’ Scale to millions of documents with native indexing


[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m
[36mRunning Go RAG Example...[0m
[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m
=== ekoDB RAG Conversation System ===

This example shows how ekoDB can power a self-improving AI system
that learns from its own conversation history.

=== Step 1: Building Conversation History ===
Storing previous conversations with embeddings...

  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 34 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.448s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 169 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.499s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 33 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.274s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 230 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.326s
    â€¢ Function auto-cleaned up by client
âœ“ Stored Rust programming conversation (4 messages)
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 31 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.296s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 217 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.379s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 33 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.357s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 232 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.331s
    â€¢ Function auto-cleaned up by client
âœ“ Stored database design conversation (4 messages)
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 36 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.430s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 178 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.386s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 37 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.373s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 213 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.435s
    â€¢ Function auto-cleaned up by client
âœ“ Stored performance optimization conversation (4 messages)

=== Step 2: New User Question with Context Retrieval ===
User asks: "How do I write memory-safe high-performance database code?"

=== Step 3: Searching Related Context ===
Using hybrid search to find relevant messages from all conversations...


â†’ Generating embedding for user question...
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 58 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.326s
    â€¢ Function auto-cleaned up by client

â†’ Executing HybridSearch()...
  â€¢ Collection: rag_messages
  â€¢ Query text: "How do I write memory-safe high-performance database code?"
  â€¢ Vector dimensions: 1536
  â€¢ Limit: 5 results
  â€¢ Search type: Semantic (vector) + Keyword (text)
  â€¢ Server combines both scores for relevance ranking
  âœ“ Search completed in 0.076s

âœ“ Found 5 related messages across all conversations:
  1. [Score: 0.000] From conv_database_design
     Database normalization is the process of organizing data to reduce redundancy and improve data integrity. It involves dividing large tables into smaller ones and defining relationships between them using foreign keys.

  2. [Score: 0.000] From conv_database_design
     What is database normalization?

  3. [Score: 0.000] From conv_database_design
     Use NoSQL when you need: flexible schemas, horizontal scaling, high write throughput, or when working with unstructured data. SQL is better for complex queries, ACID transactions, and structured data with well-defined relationships.

  4. [Score: 0.000] From conv_database_design
     When should I use NoSQL over SQL?

  5. [Score: 0.000] From conv_performance
     How can I optimize database queries?

=== Step 4: Generating Context-Aware Response ===
âœ“ AI Response (with context from 3 conversations):

Writing memory-safe, high-performance database code involves a variety of best practices that touch on database design, querying strategy, and programming techniques. Here are some guidelines that can help:

1. **Database Normalization**: As discussed in Context 1, database normalization is an important process for organizing data to reduce redundancy and improve data integrity. By dividing large tables into smaller ones and defining relationships between them using foreign keys, you can optimize the storage of data and make queries more efficient.

2. **Choose the Right Database Type**: As discussed in Context 3 and 4, the choice between SQL and NoSQL databases can significantly affect performance. SQL databases are better for complex queries, ACID transactions, and structured data with well-defined relationships, while NoSQL databases are better for flexible schemas, horizontal scaling, high write throughput, and working with unstructured data. Select the type of database that best suits your use case.

3. **Optimize Queries**: As asked in Context 5, optimizing queries is crucial for high-performance database code. This can involve techniques like:
   - Using indexes to speed up queries.
   - Minimizing the amount of data that needs to be read by the query.
   - Avoiding unnecessary joins, and when joins are necessary, ensuring that they are performed on indexed columns.
   - Using the EXPLAIN command to understand how your database is executing the query and identify potential bottlenecks.

4. **Memory Management in Programming**: When writing the code that interacts with your database, it's essential to manage memory efficiently. This involves:
   - Using appropriate data structures that minimize memory usage.
   - Ensuring objects are properly disposed of or dereferenced when no longer needed to avoid memory leaks.
   - If you're using a language with manual memory management like C or C++, ensure you're free-ing any allocated memory that's no longer needed.

5. **Concurrency and Connection Management**: Managing database connections efficiently can have a big impact on performance. Avoid opening and closing connections repeatedly within a short period of time, as this can create unnecessary overhead. Also, consider using connection pooling. Handle concurrency appropriately to avoid deadlock situations.

6. **Use Batch Operations**: Whenever possible, try to use batch operations instead of single-row CRUD operations. This can significantly reduce the number of round-trip calls between your application and the database, improving performance.

Remember, the best practices can vary depending on the specific requirements of your project, the database system you're using, and the programming language you're writing your code in.

=== Step 5: Storing New Conversation ===
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 58 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.380s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 2779 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.409s
    â€¢ Function auto-cleaned up by client
âœ“ New conversation stored and indexed for future retrieval

=== Step 6: Cross-Conversation Search ===
Searching for messages about 'ownership' across ALL conversations...


â†’ Executing TextSearch()...
  â€¢ Collection: rag_messages
  â€¢ Query: "ownership system"
  â€¢ Limit: 3 results
  â€¢ Search method: Full-text with fuzzy matching & stemming
  â€¢ No vector embeddings needed - pure keyword search
  âœ“ Text search completed in 0.066s

âœ“ Found 3 messages mentioning ownership:
  1. From conv_rust_programming: Rust's key features include: memory safety without garbage collection, zero-cost abstractions, ownership system, powerful type system, and excellent concurrency support.

  2. From conv_performance: Rust's ownership system provides zero-cost memory management. Use Box for heap allocation, Rc/Arc for shared ownership, and avoid cloning large data structures. The compiler optimizes away unnecessary allocations.

  3. From conv_new_question: Writing memory-safe, high-performance database code involves a variety of best practices that touch on database design, querying strategy, and programming techniques. Here are some guidelines that can help:

1. **Database Normalization**: As discussed in Context 1, database normalization is an important process for organizing data to reduce redundancy and improve data integrity. By dividing large tables into smaller ones and defining relationships between them using foreign keys, you can optimize the storage of data and make queries more efficient.

2. **Choose the Right Database Type**: As discussed in Context 3 and 4, the choice between SQL and NoSQL databases can significantly affect performance. SQL databases are better for complex queries, ACID transactions, and structured data with well-defined relationships, while NoSQL databases are better for flexible schemas, horizontal scaling, high write throughput, and working with unstructured data. Select the type of database that best suits your use case.

3. **Optimize Queries**: As asked in Context 5, optimizing queries is crucial for high-performance database code. This can involve techniques like:
   - Using indexes to speed up queries.
   - Minimizing the amount of data that needs to be read by the query.
   - Avoiding unnecessary joins, and when joins are necessary, ensuring that they are performed on indexed columns.
   - Using the EXPLAIN command to understand how your database is executing the query and identify potential bottlenecks.

4. **Memory Management in Programming**: When writing the code that interacts with your database, it's essential to manage memory efficiently. This involves:
   - Using appropriate data structures that minimize memory usage.
   - Ensuring objects are properly disposed of or dereferenced when no longer needed to avoid memory leaks.
   - If you're using a language with manual memory management like C or C++, ensure you're free-ing any allocated memory that's no longer needed.

5. **Concurrency and Connection Management**: Managing database connections efficiently can have a big impact on performance. Avoid opening and closing connections repeatedly within a short period of time, as this can create unnecessary overhead. Also, consider using connection pooling. Handle concurrency appropriately to avoid deadlock situations.

6. **Use Batch Operations**: Whenever possible, try to use batch operations instead of single-row CRUD operations. This can significantly reduce the number of round-trip calls between your application and the database, improving performance.

Remember, the best practices can vary depending on the specific requirements of your project, the database system you're using, and the programming language you're writing your code in.

=== System Statistics ===

â†’ Querying database statistics...
  â€¢ Using FindAll() helper - simplified query API

ğŸ“Š Database Statistics:
  â€¢ Total conversations: 4
  â€¢ Total messages stored: 14
  â€¢ All messages indexed for vector search âœ“
  â€¢ All messages indexed for text search âœ“
  â€¢ All messages queryable by metadata âœ“

=== Step 8: Dynamic Search Configuration ===
Each conversation can have its own search config...

ğŸ’¡ Conversations can store custom search configurations:
  â€¢ Search type: hybrid, text, or vector
  â€¢ Relevance thresholds
  â€¢ Filter by tags or metadata
  â€¢ Collection-specific settings
  â€¢ Per-conversation AI behavior

This enables context-aware search tuned to each conversation's needs!

=== Cleanup ===
âœ“ Cleanup complete


=== ğŸ“š Summary: What This Example Showed ===

ğŸ”§ ekoDB Native Capabilities Used:
  âœ“ Functions with Embed operation (AI integration)
  âœ“ Hybrid Search (text + vector combined)
  âœ“ Text Search (full-text with stemming)
  âœ“ Automatic embedding generation
  âœ“ Cross-collection queries

ğŸš€ New Client Helper Methods:
  â€¢ client.Embed(text, model) - Generate embeddings
  â€¢ client.HybridSearch() - Semantic + keyword search
  â€¢ client.TextSearch() - Full-text search
  â€¢ client.FindAll() - Query all documents

ğŸ’¡ Key Takeaways:
  1. ekoDB handles AI Functions natively - no external services needed
  2. One-line embedding generation with auto-cleanup
  3. Hybrid search combines semantic understanding + keyword matching
  4. Perfect for RAG: store, search, and retrieve context
  5. All AI capabilities accessible through simple client methods

ğŸ¯ Build production RAG systems with ekoDB!
   â†’ Set OPENAI_API_KEY in your ekoDB server environment
   â†’ Use these client helpers to make AI integration simple
   â†’ Scale to millions of documents with native indexing


[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m
[36mRunning Kotlin RAG Example...[0m
[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m
=== ekoDB RAG Conversation System ===

This example shows how ekoDB can power a self-improving AI system
that learns from its own conversation history.

SLF4J(W): No SLF4J providers were found.
SLF4J(W): Defaulting to no-operation (NOP) logger implementation
SLF4J(W): See https://www.slf4j.org/codes.html#noProviders for further details.
=== Step 1: Building Conversation History ===
Storing previous conversations with embeddings...

  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 34 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 2.41s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 169 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.512s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 33 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.28s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 230 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.348s
    â€¢ Function auto-cleaned up by client
âœ“ Stored Rust programming conversation (4 messages)
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 31 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.482s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 217 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.343s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 33 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.239s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 232 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.33s
    â€¢ Function auto-cleaned up by client
âœ“ Stored database design conversation (4 messages)
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 36 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.57s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 178 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.539s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 37 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.351s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 213 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.337s
    â€¢ Function auto-cleaned up by client
âœ“ Stored performance optimization conversation (4 messages)

=== Step 2: New User Question with Context Retrieval ===
User asks: "How do I write memory-safe high-performance database code?"

=== Step 3: Searching Related Context ===
Using hybrid search to find relevant messages from all conversations...


â†’ Generating embedding for user question...
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 58 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.615s
    â€¢ Function auto-cleaned up by client

â†’ Executing hybridSearch()...
  â€¢ Collection: rag_messages
  â€¢ Query text: "How do I write memory-safe high-performance database code?"
  â€¢ Vector dimensions: 1536
  â€¢ Limit: 5 results
  â€¢ Search type: Semantic (vector) + Keyword (text)
  â€¢ Server combines both scores for relevance ranking
  âœ“ Search completed in 0.086s

âœ“ Found 5 related messages across all conversations:
  1. [Score: 0.000] From conv_performance
     How can I optimize database queries?

  2. [Score: 0.000] From conv_database_design
     Use NoSQL when you need: flexible schemas, horizontal scaling, high write throughput, or when working with unstructured data. SQL is better for complex queries, ACID transactions, and structured data with well-defined relationships.

  3. [Score: 0.000] From conv_database_design
     When should I use NoSQL over SQL?

  4. [Score: 0.000] From conv_database_design
     What is database normalization?

  5. [Score: 0.000] From conv_database_design
     Database normalization is the process of organizing data to reduce redundancy and improve data integrity. It involves dividing large tables into smaller ones and defining relationships between them using foreign keys.

=== Step 4: Generating Context-Aware Response ===
âœ“ Context prepared from search results
âœ“ AI would use this context to generate comprehensive response

=== Step 5: Storing New Conversation ===
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 58 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.437s
    â€¢ Function auto-cleaned up by client
âœ“ New conversation stored and indexed for future retrieval

=== Step 6: Cross-Conversation Search ===
Searching for messages about 'ownership' across ALL conversations...


â†’ Executing textSearch()...
  â€¢ Collection: rag_messages
  â€¢ Query: "ownership system"
  â€¢ Limit: 3 results
  â€¢ Search method: Full-text with fuzzy matching & stemming
  â€¢ No vector embeddings needed - pure keyword search
  âœ“ Text search completed in 0.054s

âœ“ Found 3 messages mentioning ownership:
  1. From conv_rust_programming: Rust's key features include: memory safety without garbage collection, zero-cost abstractions, ownership system, powerful type system, and excellent concurrency support.

  2. From conv_performance: Rust's ownership system provides zero-cost memory management. Use Box for heap allocation, Rc/Arc for shared ownership, and avoid cloning large data structures. The compiler optimizes away unnecessary allocations.

  3. From conv_rust_programming: The borrow checker enforces Rust's ownership rules at compile time. It ensures that references don't outlive the data they point to and prevents data races by allowing either multiple immutable references or one mutable reference.

=== System Statistics ===

â†’ Querying database statistics...
  â€¢ Using findAllWithLimit() helper - simplified query API

ğŸ“Š Database Statistics:
  â€¢ Total conversations: 4
  â€¢ Total messages stored: 13
  â€¢ All messages indexed for vector search âœ“
  â€¢ All messages indexed for text search âœ“
  â€¢ All messages queryable by metadata âœ“

=== Step 8: Dynamic Search Configuration ===
Each conversation can have its own search config...

ğŸ’¡ Conversations can store custom search configurations:
  â€¢ Search type: hybrid, text, or vector
  â€¢ Relevance thresholds
  â€¢ Filter by tags or metadata
  â€¢ Collection-specific settings
  â€¢ Per-conversation AI behavior

This enables context-aware search tuned to each conversation's needs!

=== Cleanup ===
âœ“ Cleanup complete


=== ğŸ“š Summary: What This Example Showed ===

ğŸ”§ ekoDB Native Capabilities Used:
  âœ“ Functions with Embed operation (AI integration)
  âœ“ Hybrid Search (text + vector combined)
  âœ“ Text Search (full-text with stemming)
  âœ“ Automatic embedding generation
  âœ“ Cross-collection queries

ğŸš€ New Client Helper Methods:
  â€¢ client.embed(text, model) - Generate embeddings
  â€¢ client.hybridSearch() - Semantic + keyword search
  â€¢ client.textSearch() - Full-text search
  â€¢ client.findAllWithLimit() - Query all documents

ğŸ’¡ Key Takeaways:
  1. ekoDB handles AI Functions natively - no external services needed
  2. One-line embedding generation with auto-cleanup
  3. Hybrid search combines semantic understanding + keyword matching
  4. Perfect for RAG: store, search, and retrieve context
  5. All AI capabilities accessible through simple client methods

ğŸ¯ Build production RAG systems with ekoDB!
   â†’ Set OPENAI_API_KEY in your ekoDB server environment
   â†’ Use these client helpers to make AI integration simple
   â†’ Scale to millions of documents with native indexing


[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m
âœ… [32mRAG Examples Complete![0m
[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m

[32mWhat you just saw across 5 languages:[0m
  âœ“ Embeddings generated via ekoDB Functions
  âœ“ Hybrid search (semantic + keyword)
  âœ“ Text search with stemming
  âœ“ Cross-conversation context retrieval
  âœ“ Simple client helpers wrapping powerful AI

[36mMission: AI for All ğŸš€[0m - Making RAG accessible to everyone!

