make test-examples-rag

ğŸ¤– [36mRAG Conversation System Examples[0m
[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m

[33mPrerequisites:[0m
  1. ekoDB server running (make run in ekodb/)
  2. OPENAI_API_KEY set in server environment
  3. API_BASE_URL and API_BASE_KEY exported in your shell

[36mBuilding Rust client library...[0m
âœ“ Rust client built

[36mBuilding Python client bindings...[0m
ğŸ¹ Building a mixed python/rust project
ğŸ”— Found pyo3 bindings with abi3 support
ğŸ Not using a specific python interpreter
ğŸ’» Using `MACOSX_DEPLOYMENT_TARGET=11.0` for aarch64-apple-darwin by default
ğŸ“¦ Built wheel for abi3 Python â‰¥ 3.8 to /Users/tek/Development/ekoDB/ekodb-client/ekodb-client-py/target/wheels/ekodb_client-0.7.1-cp38-abi3-macosx_11_0_arm64.whl
âœ“ Python client built and installed

[36mBuilding TypeScript client library...[0m
âœ“ TypeScript client built

[36mInstalling TypeScript client in examples...[0m
âœ“ TypeScript client installed

[36mBuilding TypeScript example...[0m
âœ“ TypeScript example built

[36mBuilding Go client library...[0m
âœ“ Go client built

[36mBuilding Go RAG example...[0m
âœ“ Go example built

[36mBuilding Kotlin client library...[0m
âœ“ Kotlin client built

[36mBuilding Kotlin RAG example...[0m
âœ“ Kotlin example built

[36mRunning Rust RAG Example...[0m
[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m
=== ekoDB RAG Conversation System ===

This example shows how ekoDB can power a self-improving AI system
that learns from its own conversation history.

=== Step 1: Building Conversation History ===
Storing previous conversations with embeddings...

  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 34 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 408.946291ms
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 169 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 1.010538875s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 33 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 280.850584ms
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 230 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 1.136472625s
    â€¢ Function auto-cleaned up by client
âœ“ Stored Rust programming conversation (4 messages)
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 31 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 1.116152292s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 217 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 948.992708ms
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 33 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 365.4545ms
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 232 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 1.010931708s
    â€¢ Function auto-cleaned up by client
âœ“ Stored database design conversation (4 messages)
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 36 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 944.366125ms
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 178 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 1.193054834s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 37 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 251.800458ms
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 213 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 445.393292ms
    â€¢ Function auto-cleaned up by client
âœ“ Stored performance optimization conversation (4 messages)

=== Step 2: New User Question with Context Retrieval ===
User asks: "How do I write memory-safe high-performance database code?"

=== Step 3: Searching Related Context ===
Using hybrid search to find relevant messages from all conversations...


â†’ Generating embedding for user question...
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 58 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 401.4465ms
    â€¢ Function auto-cleaned up by client

â†’ Executing hybrid_search()...
  â€¢ Collection: rag_messages
  â€¢ Query text: "How do I write memory-safe high-performance database code?"
  â€¢ Vector dimensions: 1536
  â€¢ Limit: 5 results
  â€¢ Search type: Semantic (vector) + Keyword (text)
  â€¢ Server combines both scores for relevance ranking
  âœ“ Search completed in 102.272625ms

âœ“ Found 5 related messages across all conversations:
  1. From conv_database_design
     Use NoSQL when you need: flexible schemas, horizontal scaling, high write throughput, or when working with unstructured data. SQL is better for complex queries, ACID transactions, and structured data with well-defined relationships.

  2. From conv_database_design
     What is database normalization?

  3. From conv_database_design
     Database normalization is the process of organizing data to reduce redundancy and improve data integrity. It involves dividing large tables into smaller ones and defining relationships between them using foreign keys.

  4. From conv_database_design
     When should I use NoSQL over SQL?

  5. From conv_performance
     How can I optimize database queries?

=== Step 4: Generating Context-Aware Response ===
âœ“ AI Response (with context from 3 conversations):

Writing memory-safe, high-performance database code involves several steps:

1. **Minimize Data Volume**: Aim to only pull the data you need. Extraneous data can increase memory usage and slow down your queries. This is where database normalization, as mentioned in Context 3, can be beneficial. It helps to reduce redundancy, thus minimizing the volume of data.

2. **Optimize Queries**: Properly optimizing your SQL or NoSQL queries can greatly improve performance. Use indexing, limit the number of joins, or reduce subqueries wherever possible. Make sure to analyze and understand your query execution plans.

3. **Implement Efficient Data Structures and Algorithms**: The choice of data structures and algorithms can significantly impact memory usage and performance. 

4. **Connection Pooling**: Reuse database connections instead of creating new ones every time. Creating new database connections can be expensive and slow down your application.

5. **Caching**: Cache your most frequently accessed data. This can reduce the load on your database and make your application faster.

6. **Concurrency Control**: If you are using a SQL database and need to handle ACID transactions, proper concurrency control is important for both performance and data integrity. 

7. **Use Appropriate Database**: As mentioned in Context 1 and Context 4, the choice between SQL and NoSQL can affect performance. NoSQL databases can be a better choice for applications that require high write throughput, horizontal scaling, or have unstructured data. However, SQL databases are often better for complex queries or data with well-defined relationships.

8. **Memory Management**: In the context of the programming language you are using, ensure you are following best practices for memory management to avoid memory leaks or unnecessary memory consumption.

Remember, the key is to understand your specific use case and workload, as different strategies may be more effective in different situations.

=== Step 5: Storing New Conversation ===
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 58 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 427.039ms
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 1988 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 253.557583ms
    â€¢ Function auto-cleaned up by client
âœ“ New conversation stored and indexed for future retrieval

=== Step 6: Cross-Conversation Search ===
Searching for messages about 'ownership' across ALL conversations...


â†’ Executing text_search()...
  â€¢ Collection: rag_messages
  â€¢ Query: "ownership system"
  â€¢ Limit: 3 results
  â€¢ Search method: Full-text with fuzzy matching & stemming
  â€¢ No vector embeddings needed - pure keyword search
  âœ“ Text search completed in 61.037417ms

âœ“ Found 3 messages mentioning ownership:
  1. From conv_rust_programming: Rust's key features include: memory safety without garbage collection, zero-cost abstractions, ownership system, powerful type system, and excellent concurrency support.

  2. From conv_performance: Rust's ownership system provides zero-cost memory management. Use Box for heap allocation, Rc/Arc for shared ownership, and avoid cloning large data structures. The compiler optimizes away unnecessary allocations.

  3. From conv_rust_programming: The borrow checker enforces Rust's ownership rules at compile time. It ensures that references don't outlive the data they point to and prevents data races by allowing either multiple immutable references or one mutable reference.

=== System Statistics ===

â†’ Querying database statistics...
  â€¢ Using find_all() helper - simplified query API

ğŸ“Š Database Statistics:
  â€¢ Total conversations: 4
  â€¢ Total messages stored: 14
  â€¢ All messages indexed for vector search âœ“
  â€¢ All messages indexed for text search âœ“
  â€¢ All messages queryable by metadata âœ“

=== Step 8: Dynamic Search Configuration ===
Each conversation can have its own search config...

ğŸ’¡ Conversations can store custom search configurations:
  â€¢ Search type: hybrid, text, or vector
  â€¢ Relevance thresholds
  â€¢ Filter by tags or metadata
  â€¢ Collection-specific settings
  â€¢ Per-conversation AI behavior

This enables context-aware search tuned to each conversation's needs!


=== Cleanup ===
Deleting example collections...

âœ… All done! RAG system demonstrated successfully.

âœ“ Using search results to enhance AI responses (RAG)
âœ“ Building a self-improving knowledge base
âœ“ Dynamic search configurations per conversation

ekoDB provides everything needed for AI-powered applications:
  â€¢ Vector search (semantic similarity)
  â€¢ Text search (keyword matching)
  â€¢ Hybrid search (best of both)
  â€¢ AI functions (Chat, Embed)
  â€¢ Flexible querying and filtering
  â€¢ All in one database - no external dependencies!


[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m
[36mRunning Python RAG Example...[0m
[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m
=== ekoDB RAG Conversation System ===

This example shows how ekoDB can power a self-improving AI system
that learns from its own conversation history.

=== Step 1: Building Conversation History ===
Storing previous conversations with embeddings...

  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 34 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.793s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 169 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 1.315s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 33 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.302s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 230 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.501s
    â€¢ Function auto-cleaned up by client
âœ“ Stored Rust programming conversation (4 messages)
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 31 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.247s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 217 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.193s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 33 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.450s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 232 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.310s
    â€¢ Function auto-cleaned up by client
âœ“ Stored database design conversation (4 messages)
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 36 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.259s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 178 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.326s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 37 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.388s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 213 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.231s
    â€¢ Function auto-cleaned up by client
âœ“ Stored performance optimization conversation (4 messages)

=== Step 2: New User Question with Context Retrieval ===
User asks: "How do I write memory-safe high-performance database code?"

=== Step 3: Searching Related Context ===
Using hybrid search to find relevant messages from all conversations...


â†’ Generating embedding for user question...
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 58 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.201s
    â€¢ Function auto-cleaned up by client

â†’ Executing hybrid_search()...
  â€¢ Collection: rag_messages
  â€¢ Query text: "How do I write memory-safe high-performance database code?"
  â€¢ Vector dimensions: 1536
  â€¢ Limit: 5 results
  â€¢ Search type: Semantic (vector) + Keyword (text)
  â€¢ Server combines both scores for relevance ranking
  âœ“ Search completed in 0.092s

âœ“ Found 5 related messages across all conversations:
  1. [Score: 0.000] From conv_database_design
     Use NoSQL when you need: flexible schemas, horizontal scaling, high write throughput, or when working with unstructured data. SQL is better for complex queries, ACID transactions, and structured data with well-defined relationships.

  2. [Score: 0.000] From conv_database_design
     What is database normalization?

  3. [Score: 0.000] From conv_database_design
     Database normalization is the process of organizing data to reduce redundancy and improve data integrity. It involves dividing large tables into smaller ones and defining relationships between them using foreign keys.

  4. [Score: 0.000] From conv_database_design
     When should I use NoSQL over SQL?

  5. [Score: 0.000] From conv_performance
     How can I optimize database queries?

=== Step 4: Generating Context-Aware Response ===
âœ“ AI Response (with context from 3 conversations):

Writing memory-safe, high-performance database code involves several practices:

1. **Use Appropriate Data Structures:** Data structures greatly influence memory usage and performance. For example, in many applications, using a hash map for rapid lookups can be more efficient than using an array or a list.

2. **Normalization:** As mentioned in Context 3, database normalization can reduce data redundancy and improve data integrity. This can enhance the performance of your database as it avoids unnecessary data and ensures that the database structure is efficient.

3. **Optimize Queries:** As per Context 5, optimizing your database queries can greatly improve performance. This may include techniques such as using indexes effectively, avoiding unnecessary joins, and minimizing the use of functions in WHERE clauses.

4. **Memory Management:** In languages that allow manual memory management, ensure you are handling memory allocation and deallocation properly to avoid memory leaks and crashes. In garbage-collected languages, be mindful of how your code can affect the garbage collector and potentially slow down your application.

5. **Use Prepared Statements:** Using prepared statements can both improve performance and protect against SQL injection attacks. Prepared statements can be compiled once and run many times, which can make repeated queries faster.

6. **Use Efficient Transactions:** Transactions can lock database tables and cause performance issues. Using transactions efficiently â€” for example, by avoiding long-running transactions â€” can help ensure high performance.

7. **Use Appropriate Database Type:** Depending on your needs, using SQL or NoSQL can affect the performance of your database. As per Context 1 and 4, NoSQL is better for handling big data with high write throughput and flexible schemas, while SQL is better for structured data with well-defined relationships and ACID transactions.

Remember, the choice of programming language, database type, and whether to use a ORM or raw SQL can also affect the memory safety and performance of your database code. Always measure performance and memory usage in realistic scenarios, and make informed decisions based on those measurements.

=== Step 5: Storing New Conversation ===
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 58 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.312s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 2227 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.394s
    â€¢ Function auto-cleaned up by client
âœ“ New conversation stored and indexed for future retrieval

=== Step 6: Cross-Conversation Search ===
Searching for messages about 'ownership' across ALL conversations...

âœ“ Found 3 messages mentioning ownership:
  1. From conv_performance: Rust's ownership system provides zero-cost memory management. Use Box for heap allocation, Rc/Arc for shared ownership, and avoid cloning large data structures. The compiler optimizes away unnecessary allocations.

  2. From conv_rust_programming: Rust's key features include: memory safety without garbage collection, zero-cost abstractions, ownership system, powerful type system, and excellent concurrency support.

  3. From conv_rust_programming: The borrow checker enforces Rust's ownership rules at compile time. It ensures that references don't outlive the data they point to and prevents data races by allowing either multiple immutable references or one mutable reference.

=== System Statistics ===
Total conversations: 4
Total messages stored: 14
All messages are indexed for vector search âœ“
All messages are indexed for text search âœ“
All messages are queryable by metadata âœ“

=== Step 8: Dynamic Search Configuration ===
Each conversation can have its own search config...

ğŸ’¡ Conversations can store custom search configurations:
  â€¢ Search type: hybrid, text, or vector
  â€¢ Relevance thresholds
  â€¢ Filter by tags or metadata
  â€¢ Collection-specific settings
  â€¢ Per-conversation AI behavior

This enables context-aware search tuned to each conversation's needs!

=== Cleanup ===
âœ“ Cleanup complete


=== ğŸ“š Summary: What This Example Showed ===

ğŸ”§ ekoDB Native Capabilities Used:
  âœ“ Functions with Embed operation (AI integration)
  âœ“ Hybrid Search (text + vector combined)
  âœ“ Text Search (full-text with stemming)
  âœ“ Automatic embedding generation
  âœ“ Cross-collection queries

ğŸš€ New Client Helper Methods:
  â€¢ client.embed(text, model) - Generate embeddings
  â€¢ client.hybrid_search() - Semantic + keyword search
  â€¢ client.text_search() - Full-text search
  â€¢ client.find_all() - Query all documents

ğŸ’¡ Key Takeaways:
  1. ekoDB handles AI Functions natively - no external services needed
  2. One-line embedding generation with auto-cleanup
  3. Hybrid search combines semantic understanding + keyword matching
  4. Perfect for RAG: store, search, and retrieve context
  5. All AI capabilities accessible through simple client methods

ğŸ¯ Build production RAG systems with ekoDB!
   â†’ Set OPENAI_API_KEY in your ekoDB server environment
   â†’ Use these client helpers to make AI integration simple
   â†’ Scale to millions of documents with native indexing


[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m
[36mRunning TypeScript RAG Example...[0m
[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m
=== ekoDB RAG Conversation System ===

This example shows how ekoDB can power a self-improving AI system
that learns from its own conversation history.

=== Step 1: Building Conversation History ===
Storing previous conversations with embeddings...

  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 34 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.492s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 169 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.594s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 33 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.249s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 230 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.313s
    â€¢ Function auto-cleaned up by client
âœ“ Stored Rust programming conversation (4 messages)
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 31 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.302s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 217 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.620s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 33 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.309s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 232 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.321s
    â€¢ Function auto-cleaned up by client
âœ“ Stored database design conversation (4 messages)
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 36 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.326s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 178 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.278s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 37 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.457s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 213 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.279s
    â€¢ Function auto-cleaned up by client
âœ“ Stored performance optimization conversation (4 messages)

=== Step 2: New User Question with Context Retrieval ===
User asks: "How do I write memory-safe high-performance database code?"

=== Step 3: Searching Related Context ===
Using hybrid search to find relevant messages from all conversations...


â†’ Generating embedding for user question...
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 58 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.434s
    â€¢ Function auto-cleaned up by client

â†’ Executing hybridSearch()...
  â€¢ Collection: rag_messages
  â€¢ Query text: "How do I write memory-safe high-performance database code?"
  â€¢ Vector dimensions: 1536
  â€¢ Limit: 5 results
  â€¢ Search type: Semantic (vector) + Keyword (text)
  â€¢ Server combines both scores for relevance ranking
  âœ“ Search completed in 0.077s
âœ“ Found 5 related messages across all conversations:
  1. [Score: 0.000] From conv_performance
     How can I optimize database queries?

  2. [Score: 0.000] From conv_database_design
     Use NoSQL when you need: flexible schemas, horizontal scaling, high write throughput, or when working with unstructured data. SQL is better for complex queries, ACID transactions, and structured data with well-defined relationships.

  3. [Score: 0.000] From conv_database_design
     When should I use NoSQL over SQL?

  4. [Score: 0.000] From conv_database_design
     What is database normalization?

  5. [Score: 0.000] From conv_database_design
     Database normalization is the process of organizing data to reduce redundancy and improve data integrity. It involves dividing large tables into smaller ones and defining relationships between them using foreign keys.

=== Step 4: Generating Context-Aware Response ===
âœ“ AI Response (with context from 3 conversations):

Writing memory-safe, high-performance database code involves a combination of various techniques and best practices. Here are a few steps you can follow:

1. **Optimize your queries**: As mentioned in Context 1, optimizing your database queries can significantly improve the performance of your application. This can include limiting the amount of data returned from a query, avoiding nested queries, using joins instead of multiple queries, and using indexed columns for searches.

2. **Choose the right database**: Depending on your requirements, either SQL or NoSQL may be a better fit. As described in Context 2 and 3, NoSQL is typically better for flexible schemas, horizontal scaling, high write throughput, and unstructured data, while SQL is better for complex queries, ACID transactions, and structured data with well-defined relationships.

3. **Normalize your database**: As explained in Context 4 and 5, database normalization can help to reduce redundancy and improve data integrity. By dividing large tables into smaller ones and defining relationships between them using foreign keys, you can ensure that your data is organized in a way that is efficient and easy to query.

4. **Use proper memory management techniques**: In most programming languages, you need to properly manage the memory used by your application. This can include freeing up memory that is no longer in use, preventing memory leaks, and handling out-of-memory errors. This is especially important when dealing with large amounts of data.

5. **Concurrency control**: When multiple users are accessing and modifying the data concurrently, you need to ensure that this doesn't lead to inconsistencies or conflicts. Techniques such as locking, transactions, or optimistic concurrency control can be used to handle this.

6. **Use caching**: Caching can also improve the performance of your database code by storing the results of common queries in memory, so that they can be returned more quickly the next time the same query is run.

7. **Batch processing**: Instead of processing each database operation individually, you can often improve performance by batching operations together, especially when inserting or updating large amounts of data.

Remember that every database and application is unique, so these are general guidelines that may not apply or may need to be modified depending on your specific context.

=== Step 5: Storing New Conversation ===
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 58 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.389s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 2403 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.373s
    â€¢ Function auto-cleaned up by client
âœ“ New conversation stored and indexed for future retrieval

=== Step 6: Cross-Conversation Search ===
Searching for messages about 'ownership' across ALL conversations...


â†’ Executing textSearch()...
  â€¢ Collection: rag_messages
  â€¢ Query: "ownership system"
  â€¢ Limit: 3 results
  â€¢ Search method: Full-text with fuzzy matching & stemming
  â€¢ No vector embeddings needed - pure keyword search
  âœ“ Text search completed in 0.052s
âœ“ Found 3 messages mentioning ownership:
  1. From conv_rust_programming: Rust's key features include: memory safety without garbage collection, zero-cost abstractions, ownership system, powerful type system, and excellent concurrency support.

  2. From conv_performance: Rust's ownership system provides zero-cost memory management. Use Box for heap allocation, Rc/Arc for shared ownership, and avoid cloning large data structures. The compiler optimizes away unnecessary allocations.

  3. From conv_rust_programming: The borrow checker enforces Rust's ownership rules at compile time. It ensures that references don't outlive the data they point to and prevents data races by allowing either multiple immutable references or one mutable reference.

=== System Statistics ===

â†’ Querying database statistics...
  â€¢ Using findAllWithLimit() helper - simplified query API

ğŸ“Š Database Statistics:
  â€¢ Total conversations: 4
  â€¢ Total messages stored: 14
  â€¢ All messages indexed for vector search âœ“
  â€¢ All messages indexed for text search âœ“
  â€¢ All messages queryable by metadata âœ“

=== Step 8: Dynamic Search Configuration ===
Each conversation can have its own search config...

ğŸ’¡ Conversations can store custom search configurations:
  â€¢ Search type: hybrid, text, or vector
  â€¢ Relevance thresholds
  â€¢ Filter by tags or metadata
  â€¢ Collection-specific settings
  â€¢ Per-conversation AI behavior

This enables context-aware search tuned to each conversation's needs!

=== Cleanup ===
âœ“ Cleanup complete


=== ğŸ“š Summary: What This Example Showed ===

ğŸ”§ ekoDB Native Capabilities Used:
  âœ“ Functions with Embed operation (AI integration)
  âœ“ Hybrid Search (text + vector combined)
  âœ“ Text Search (full-text with stemming)
  âœ“ Automatic embedding generation
  âœ“ Cross-collection queries

ğŸš€ New Client Helper Methods:
  â€¢ client.embed(text, model) - Generate embeddings
  â€¢ client.hybridSearch() - Semantic + keyword search
  â€¢ client.textSearch() - Full-text search
  â€¢ client.findAllWithLimit() - Query all documents

ğŸ’¡ Key Takeaways:
  1. ekoDB handles AI Functions natively - no external services needed
  2. One-line embedding generation with auto-cleanup
  3. Hybrid search combines semantic understanding + keyword matching
  4. Perfect for RAG: store, search, and retrieve context
  5. All AI capabilities accessible through simple client methods

ğŸ¯ Build production RAG systems with ekoDB!
   â†’ Set OPENAI_API_KEY in your ekoDB server environment
   â†’ Use these client helpers to make AI integration simple
   â†’ Scale to millions of documents with native indexing


[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m
[36mRunning Go RAG Example...[0m
[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m
=== ekoDB RAG Conversation System ===

This example shows how ekoDB can power a self-improving AI system
that learns from its own conversation history.

=== Step 1: Building Conversation History ===
Storing previous conversations with embeddings...

  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 34 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.301s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 169 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.309s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 33 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.326s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 230 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.261s
    â€¢ Function auto-cleaned up by client
âœ“ Stored Rust programming conversation (4 messages)
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 31 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.408s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 217 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.249s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 33 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.290s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 232 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.275s
    â€¢ Function auto-cleaned up by client
âœ“ Stored database design conversation (4 messages)
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 36 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.311s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 178 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.243s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 37 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.288s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 213 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.320s
    â€¢ Function auto-cleaned up by client
âœ“ Stored performance optimization conversation (4 messages)

=== Step 2: New User Question with Context Retrieval ===
User asks: "How do I write memory-safe high-performance database code?"

=== Step 3: Searching Related Context ===
Using hybrid search to find relevant messages from all conversations...


â†’ Generating embedding for user question...
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 58 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.674s
    â€¢ Function auto-cleaned up by client

â†’ Executing HybridSearch()...
  â€¢ Collection: rag_messages
  â€¢ Query text: "How do I write memory-safe high-performance database code?"
  â€¢ Vector dimensions: 1536
  â€¢ Limit: 5 results
  â€¢ Search type: Semantic (vector) + Keyword (text)
  â€¢ Server combines both scores for relevance ranking
  âœ“ Search completed in 0.072s

âœ“ Found 5 related messages across all conversations:
  1. [Score: 0.000] From conv_performance
     How can I optimize database queries?

  2. [Score: 0.000] From conv_database_design
     Use NoSQL when you need: flexible schemas, horizontal scaling, high write throughput, or when working with unstructured data. SQL is better for complex queries, ACID transactions, and structured data with well-defined relationships.

  3. [Score: 0.000] From conv_database_design
     When should I use NoSQL over SQL?

  4. [Score: 0.000] From conv_database_design
     What is database normalization?

  5. [Score: 0.000] From conv_database_design
     Database normalization is the process of organizing data to reduce redundancy and improve data integrity. It involves dividing large tables into smaller ones and defining relationships between them using foreign keys.

=== Step 4: Generating Context-Aware Response ===
âœ“ AI Response (with context from 3 conversations):

Writing memory-safe, high-performance database code involves multiple practices and considerations. Here are some ways to do it:

1. **Optimize Database Queries**: Query optimization is critical to high-performance database code. You should always aim to write efficient queries, which means avoiding full table scans, using indexes wisely, and not fetching more data than necessary. Use 'EXPLAIN' or similar keywords to understand how your queries are being executed and optimize them accordingly.

2. **Normalization**: Use database normalization to organize data efficiently. This will reduce redundancy and improve data integrity, which can lead to improved performance and memory usage. However, remember that in some cases, a certain level of denormalization may be beneficial for query performance.

3. **Choose the Right Database Type**: Depending on your data and use-cases, you might want to choose between SQL and NoSQL databases. SQL databases are better for structured data with well-defined relationships and complex queries, while NoSQL databases are better for flexible schemas, horizontal scaling, high write throughput, or when working with unstructured data.

4. **Use Prepared Statements**: Prepared statements not only enhance security but also improve performance as they help in reducing the amount of time spent on query parsing, compiling, and optimization during execution.

5. **Proper Memory Management**: In your code, always ensure to close database connections and free up any resources that are no longer needed. This prevents memory leaks and keeps your application's memory footprint under control.

6. **Caching**: Implement caching wherever possible. This reduces the load on the database and helps to retrieve frequent data more quickly, which improves overall performance.

7. **Batch Operations**: If you have multiple similar operations to perform, batch them together. This reduces the amount of communication between your code and the database, which can significantly improve performance.  

8. **Concurrency Control**: Implement appropriate concurrency control mechanisms to ensure that multiple operations do not conflict with each other. This is particularly important in multi-user systems where multiple users might be reading and writing to the database simultaneously.

Remember, high-performance database code is not just about speed. It's about ensuring that your application can handle its workload efficiently and reliably, without compromising data integrity or security.

=== Step 5: Storing New Conversation ===
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 58 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.397s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 2526 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.416s
    â€¢ Function auto-cleaned up by client
âœ“ New conversation stored and indexed for future retrieval

=== Step 6: Cross-Conversation Search ===
Searching for messages about 'ownership' across ALL conversations...


â†’ Executing TextSearch()...
  â€¢ Collection: rag_messages
  â€¢ Query: "ownership system"
  â€¢ Limit: 3 results
  â€¢ Search method: Full-text with fuzzy matching & stemming
  â€¢ No vector embeddings needed - pure keyword search
  âœ“ Text search completed in 0.053s

âœ“ Found 3 messages mentioning ownership:
  1. From conv_performance: Rust's ownership system provides zero-cost memory management. Use Box for heap allocation, Rc/Arc for shared ownership, and avoid cloning large data structures. The compiler optimizes away unnecessary allocations.

  2. From conv_rust_programming: Rust's key features include: memory safety without garbage collection, zero-cost abstractions, ownership system, powerful type system, and excellent concurrency support.

  3. From conv_rust_programming: The borrow checker enforces Rust's ownership rules at compile time. It ensures that references don't outlive the data they point to and prevents data races by allowing either multiple immutable references or one mutable reference.

=== System Statistics ===

â†’ Querying database statistics...
  â€¢ Using FindAll() helper - simplified query API

ğŸ“Š Database Statistics:
  â€¢ Total conversations: 4
  â€¢ Total messages stored: 14
  â€¢ All messages indexed for vector search âœ“
  â€¢ All messages indexed for text search âœ“
  â€¢ All messages queryable by metadata âœ“

=== Step 8: Dynamic Search Configuration ===
Each conversation can have its own search config...

ğŸ’¡ Conversations can store custom search configurations:
  â€¢ Search type: hybrid, text, or vector
  â€¢ Relevance thresholds
  â€¢ Filter by tags or metadata
  â€¢ Collection-specific settings
  â€¢ Per-conversation AI behavior

This enables context-aware search tuned to each conversation's needs!

=== Cleanup ===
âœ“ Cleanup complete


=== ğŸ“š Summary: What This Example Showed ===

ğŸ”§ ekoDB Native Capabilities Used:
  âœ“ Functions with Embed operation (AI integration)
  âœ“ Hybrid Search (text + vector combined)
  âœ“ Text Search (full-text with stemming)
  âœ“ Automatic embedding generation
  âœ“ Cross-collection queries

ğŸš€ New Client Helper Methods:
  â€¢ client.Embed(text, model) - Generate embeddings
  â€¢ client.HybridSearch() - Semantic + keyword search
  â€¢ client.TextSearch() - Full-text search
  â€¢ client.FindAll() - Query all documents

ğŸ’¡ Key Takeaways:
  1. ekoDB handles AI Functions natively - no external services needed
  2. One-line embedding generation with auto-cleanup
  3. Hybrid search combines semantic understanding + keyword matching
  4. Perfect for RAG: store, search, and retrieve context
  5. All AI capabilities accessible through simple client methods

ğŸ¯ Build production RAG systems with ekoDB!
   â†’ Set OPENAI_API_KEY in your ekoDB server environment
   â†’ Use these client helpers to make AI integration simple
   â†’ Scale to millions of documents with native indexing


[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m
[36mRunning Kotlin RAG Example...[0m
[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m
=== ekoDB RAG Conversation System ===

This example shows how ekoDB can power a self-improving AI system
that learns from its own conversation history.

SLF4J(W): No SLF4J providers were found.
SLF4J(W): Defaulting to no-operation (NOP) logger implementation
SLF4J(W): See https://www.slf4j.org/codes.html#noProviders for further details.
=== Step 1: Building Conversation History ===
Storing previous conversations with embeddings...

  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 34 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.726s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 169 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.327s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 33 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.344s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 230 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.323s
    â€¢ Function auto-cleaned up by client
âœ“ Stored Rust programming conversation (4 messages)
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 31 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.355s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 217 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.322s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 33 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.321s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 232 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.436s
    â€¢ Function auto-cleaned up by client
âœ“ Stored database design conversation (4 messages)
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 36 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 1.313s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 178 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.307s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 37 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.316s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 213 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.252s
    â€¢ Function auto-cleaned up by client
âœ“ Stored performance optimization conversation (4 messages)

=== Step 2: New User Question with Context Retrieval ===
User asks: "How do I write memory-safe high-performance database code?"

=== Step 3: Searching Related Context ===
Using hybrid search to find relevant messages from all conversations...


â†’ Generating embedding for user question...
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 58 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.26s
    â€¢ Function auto-cleaned up by client

â†’ Executing hybridSearch()...
  â€¢ Collection: rag_messages
  â€¢ Query text: "How do I write memory-safe high-performance database code?"
  â€¢ Vector dimensions: 1536
  â€¢ Limit: 5 results
  â€¢ Search type: Semantic (vector) + Keyword (text)
  â€¢ Server combines both scores for relevance ranking
  âœ“ Search completed in 0.09s

âœ“ Found 5 related messages across all conversations:
  1. [Score: 0.000] From conv_performance
     How can I optimize database queries?

  2. [Score: 0.000] From conv_database_design
     Use NoSQL when you need: flexible schemas, horizontal scaling, high write throughput, or when working with unstructured data. SQL is better for complex queries, ACID transactions, and structured data with well-defined relationships.

  3. [Score: 0.000] From conv_database_design
     When should I use NoSQL over SQL?

  4. [Score: 0.000] From conv_database_design
     What is database normalization?

  5. [Score: 0.000] From conv_database_design
     Database normalization is the process of organizing data to reduce redundancy and improve data integrity. It involves dividing large tables into smaller ones and defining relationships between them using foreign keys.

=== Step 4: Generating Context-Aware Response ===
âœ“ Context prepared from search results
âœ“ AI would use this context to generate comprehensive response

=== Step 5: Storing New Conversation ===
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 58 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.307s
    â€¢ Function auto-cleaned up by client
âœ“ New conversation stored and indexed for future retrieval

=== Step 6: Cross-Conversation Search ===
Searching for messages about 'ownership' across ALL conversations...


â†’ Executing textSearch()...
  â€¢ Collection: rag_messages
  â€¢ Query: "ownership system"
  â€¢ Limit: 3 results
  â€¢ Search method: Full-text with fuzzy matching & stemming
  â€¢ No vector embeddings needed - pure keyword search
  âœ“ Text search completed in 0.047s

âœ“ Found 3 messages mentioning ownership:
  1. From conv_rust_programming: Rust's key features include: memory safety without garbage collection, zero-cost abstractions, ownership system, powerful type system, and excellent concurrency support.

  2. From conv_performance: Rust's ownership system provides zero-cost memory management. Use Box for heap allocation, Rc/Arc for shared ownership, and avoid cloning large data structures. The compiler optimizes away unnecessary allocations.

  3. From conv_rust_programming: The borrow checker enforces Rust's ownership rules at compile time. It ensures that references don't outlive the data they point to and prevents data races by allowing either multiple immutable references or one mutable reference.

=== System Statistics ===

â†’ Querying database statistics...
  â€¢ Using findAllWithLimit() helper - simplified query API

ğŸ“Š Database Statistics:
  â€¢ Total conversations: 4
  â€¢ Total messages stored: 13
  â€¢ All messages indexed for vector search âœ“
  â€¢ All messages indexed for text search âœ“
  â€¢ All messages queryable by metadata âœ“

=== Step 8: Dynamic Search Configuration ===
Each conversation can have its own search config...

ğŸ’¡ Conversations can store custom search configurations:
  â€¢ Search type: hybrid, text, or vector
  â€¢ Relevance thresholds
  â€¢ Filter by tags or metadata
  â€¢ Collection-specific settings
  â€¢ Per-conversation AI behavior

This enables context-aware search tuned to each conversation's needs!

=== Cleanup ===
âœ“ Cleanup complete


=== ğŸ“š Summary: What This Example Showed ===

ğŸ”§ ekoDB Native Capabilities Used:
  âœ“ Functions with Embed operation (AI integration)
  âœ“ Hybrid Search (text + vector combined)
  âœ“ Text Search (full-text with stemming)
  âœ“ Automatic embedding generation
  âœ“ Cross-collection queries

ğŸš€ New Client Helper Methods:
  â€¢ client.embed(text, model) - Generate embeddings
  â€¢ client.hybridSearch() - Semantic + keyword search
  â€¢ client.textSearch() - Full-text search
  â€¢ client.findAllWithLimit() - Query all documents

ğŸ’¡ Key Takeaways:
  1. ekoDB handles AI Functions natively - no external services needed
  2. One-line embedding generation with auto-cleanup
  3. Hybrid search combines semantic understanding + keyword matching
  4. Perfect for RAG: store, search, and retrieve context
  5. All AI capabilities accessible through simple client methods

ğŸ¯ Build production RAG systems with ekoDB!
   â†’ Set OPENAI_API_KEY in your ekoDB server environment
   â†’ Use these client helpers to make AI integration simple
   â†’ Scale to millions of documents with native indexing


[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m
âœ… [32mRAG Examples Complete![0m
[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m

[32mWhat you just saw across 5 languages:[0m
  âœ“ Embeddings generated via ekoDB Functions
  âœ“ Hybrid search (semantic + keyword)
  âœ“ Text search with stemming
  âœ“ Cross-conversation context retrieval
  âœ“ Simple client helpers wrapping powerful AI

[36mMission: AI for All ğŸš€[0m - Making RAG accessible to everyone!

