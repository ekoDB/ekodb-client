make test-examples-rag

ğŸ¤– [36mRAG Conversation System Examples[0m
[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m

[33mPrerequisites:[0m

1. ekoDB server running (make run in ekodb/)
2. OPENAI_API_KEY set in server environment
3. API_BASE_URL and API_BASE_KEY exported in your shell

[36mBuilding Rust client library...[0m âœ“ Rust client built

[36mBuilding Python client bindings...[0m ğŸ¹ Building a mixed python/rust
project ğŸ”— Found pyo3 bindings with abi3 support ğŸ Not using a specific python
interpreter ğŸ’» Using `MACOSX_DEPLOYMENT_TARGET=11.0` for aarch64-apple-darwin by
default ğŸ“¦ Built wheel for abi3 Python â‰¥ 3.8 to
/Users/tek/Development/ekoDB/ekodb-client/ekodb-client-py/target/wheels/ekodb_client-0.4.0-cp38-abi3-macosx_11_0_arm64.whl
âœ“ Python client built and installed

[36mBuilding TypeScript client library...[0m âœ“ TypeScript client built

[36mInstalling TypeScript client in examples...[0m âœ“ TypeScript client installed

[36mBuilding TypeScript example...[0m âœ“ TypeScript example built

[36mBuilding Go client library...[0m âœ“ Go client built

[36mBuilding Go RAG example...[0m âœ“ Go example built

[36mBuilding Kotlin client library...[0m âœ“ Kotlin client built

[36mBuilding Kotlin RAG example...[0m âœ“ Kotlin example built

[36mRunning Rust RAG Example...[0m
[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m === ekoDB RAG
Conversation System ===

This example shows how ekoDB can power a self-improving AI system that learns
from its own conversation history.

=== Step 1: Building Conversation History === Storing previous conversations
with embeddings...

â†’ Calling ekoDB embed() helper... â€¢ Using model: text-embedding-3-small â€¢ Text
length: 34 characters â€¢ Behind the scenes: Creating temp Function with Embed
operation âœ“ Generated embedding: 1536 dimensions in 322.310666ms â€¢ Function
auto-cleaned up by client â†’ Calling ekoDB embed() helper... â€¢ Using model:
text-embedding-3-small â€¢ Text length: 169 characters â€¢ Behind the scenes:
Creating temp Function with Embed operation âœ“ Generated embedding: 1536
dimensions in 271.825834ms â€¢ Function auto-cleaned up by client â†’ Calling ekoDB
embed() helper... â€¢ Using model: text-embedding-3-small â€¢ Text length: 33
characters â€¢ Behind the scenes: Creating temp Function with Embed operation âœ“
Generated embedding: 1536 dimensions in 251.234458ms â€¢ Function auto-cleaned up
by client â†’ Calling ekoDB embed() helper... â€¢ Using model:
text-embedding-3-small â€¢ Text length: 230 characters â€¢ Behind the scenes:
Creating temp Function with Embed operation âœ“ Generated embedding: 1536
dimensions in 289.611209ms â€¢ Function auto-cleaned up by client âœ“ Stored Rust
programming conversation (4 messages) â†’ Calling ekoDB embed() helper... â€¢ Using
model: text-embedding-3-small â€¢ Text length: 31 characters â€¢ Behind the scenes:
Creating temp Function with Embed operation âœ“ Generated embedding: 1536
dimensions in 257.280167ms â€¢ Function auto-cleaned up by client â†’ Calling ekoDB
embed() helper... â€¢ Using model: text-embedding-3-small â€¢ Text length: 217
characters â€¢ Behind the scenes: Creating temp Function with Embed operation âœ“
Generated embedding: 1536 dimensions in 350.550208ms â€¢ Function auto-cleaned up
by client â†’ Calling ekoDB embed() helper... â€¢ Using model:
text-embedding-3-small â€¢ Text length: 33 characters â€¢ Behind the scenes:
Creating temp Function with Embed operation âœ“ Generated embedding: 1536
dimensions in 311.860542ms â€¢ Function auto-cleaned up by client â†’ Calling ekoDB
embed() helper... â€¢ Using model: text-embedding-3-small â€¢ Text length: 232
characters â€¢ Behind the scenes: Creating temp Function with Embed operation âœ“
Generated embedding: 1536 dimensions in 538.007833ms â€¢ Function auto-cleaned up
by client âœ“ Stored database design conversation (4 messages) â†’ Calling ekoDB
embed() helper... â€¢ Using model: text-embedding-3-small â€¢ Text length: 36
characters â€¢ Behind the scenes: Creating temp Function with Embed operation âœ“
Generated embedding: 1536 dimensions in 298.990625ms â€¢ Function auto-cleaned up
by client â†’ Calling ekoDB embed() helper... â€¢ Using model:
text-embedding-3-small â€¢ Text length: 178 characters â€¢ Behind the scenes:
Creating temp Function with Embed operation âœ“ Generated embedding: 1536
dimensions in 291.418375ms â€¢ Function auto-cleaned up by client â†’ Calling ekoDB
embed() helper... â€¢ Using model: text-embedding-3-small â€¢ Text length: 37
characters â€¢ Behind the scenes: Creating temp Function with Embed operation âœ“
Generated embedding: 1536 dimensions in 405.823958ms â€¢ Function auto-cleaned up
by client â†’ Calling ekoDB embed() helper... â€¢ Using model:
text-embedding-3-small â€¢ Text length: 213 characters â€¢ Behind the scenes:
Creating temp Function with Embed operation âœ“ Generated embedding: 1536
dimensions in 273.1395ms â€¢ Function auto-cleaned up by client âœ“ Stored
performance optimization conversation (4 messages)

=== Step 2: New User Question with Context Retrieval === User asks: "How do I
write memory-safe high-performance database code?"

=== Step 3: Searching Related Context === Using hybrid search to find relevant
messages from all conversations...

â†’ Generating embedding for user question... â†’ Calling ekoDB embed() helper... â€¢
Using model: text-embedding-3-small â€¢ Text length: 58 characters â€¢ Behind the
scenes: Creating temp Function with Embed operation âœ“ Generated embedding: 1536
dimensions in 309.944709ms â€¢ Function auto-cleaned up by client

â†’ Executing hybrid_search()... â€¢ Collection: rag_messages â€¢ Query text: "How do
I write memory-safe high-performance database code?" â€¢ Vector dimensions: 1536 â€¢
Limit: 5 results â€¢ Search type: Semantic (vector) + Keyword (text) â€¢ Server
combines both scores for relevance ranking âœ“ Search completed in 103.481625ms

âœ“ Found 5 related messages across all conversations:

1. From conv_database_design Use NoSQL when you need: flexible schemas,
   horizontal scaling, high write throughput, or when working with unstructured
   data. SQL is better for complex queries, ACID transactions, and structured
   data with well-defined relationships.

2. From conv_database_design What is database normalization?

3. From conv_database_design Database normalization is the process of organizing
   data to reduce redundancy and improve data integrity. It involves dividing
   large tables into smaller ones and defining relationships between them using
   foreign keys.

4. From conv_database_design When should I use NoSQL over SQL?

5. From conv_performance How can I optimize database queries?

=== Step 4: Generating Context-Aware Response === âœ“ AI Response (with context
from 3 conversations):

Answer: Writing memory-safe, high-performance database code involves several
steps:

1. **Understanding your data**: As mentioned in the context, it's important to
   choose the right database type (SQL or NoSQL) depending on your data's
   structure, relationships, and the type of queries you'll be performing.

2. **Normalize your data**: Normalize your data to reduce redundancy and improve
   data integrity. As stated in Context 3, normalization involves dividing large
   tables into smaller ones and defining relationships between them with foreign
   keys.

3. **Optimize your queries**: Reduce the amount of data that needs to be read
   from the database. This can be done by limiting the number of records
   returned, using WHERE clauses, indexing, and avoiding using wildcards at the
   beginning of a LIKE clause.

4. **Use prepared statements**: Prepared statements not only help prevent SQL
   injection attacks but can also enhance performance, especially when executing
   a single statement repeatedly.

5. **Connection pooling**: Create a pool of database connections and reuse them,
   rather than creating a new connection every time a client makes a request.
   This significantly reduces the overhead of establishing a new connection.

6. **Use efficient data types**: Choose the most efficient data type for your
   columns to reduce the amount of space your data consumes.

7. **Caching**: Cache the results of queries that are requested frequently and
   don't change often.

8. **Memory management**: Regularly check your application for memory leaks and
   ensure all database connections are closed when they are no longer needed.

9. **Concurrency control**: Use appropriate locking mechanisms to control the
   simultaneous access of data by multiple threads.

Remember, the performance of your database code depends largely on the specific
requirements and constraints of your project. It's essential to continuously
monitor and tune your database as those requirements evolve.

=== Step 5: Storing New Conversation === â†’ Calling ekoDB embed() helper... â€¢
Using model: text-embedding-3-small â€¢ Text length: 58 characters â€¢ Behind the
scenes: Creating temp Function with Embed operation âœ“ Generated embedding: 1536
dimensions in 284.626167ms â€¢ Function auto-cleaned up by client â†’ Calling ekoDB
embed() helper... â€¢ Using model: text-embedding-3-small â€¢ Text length: 1965
characters â€¢ Behind the scenes: Creating temp Function with Embed operation âœ“
Generated embedding: 1536 dimensions in 281.515041ms â€¢ Function auto-cleaned up
by client âœ“ New conversation stored and indexed for future retrieval

=== Step 6: Cross-Conversation Search === Searching for messages about
'ownership' across ALL conversations...

â†’ Executing text_search()... â€¢ Collection: rag_messages â€¢ Query: "ownership
system" â€¢ Limit: 3 results â€¢ Search method: Full-text with fuzzy matching &
stemming â€¢ No vector embeddings needed - pure keyword search âœ“ Text search
completed in 62.01475ms

âœ“ Found 3 messages mentioning ownership:

1. From conv_performance: Rust's ownership system provides zero-cost memory
   management. Use Box for heap allocation, Rc/Arc for shared ownership, and
   avoid cloning large data structures. The compiler optimizes away unnecessary
   allocations.

2. From conv_rust_programming: Rust's key features include: memory safety
   without garbage collection, zero-cost abstractions, ownership system,
   powerful type system, and excellent concurrency support.

3. From conv_rust_programming: The borrow checker enforces Rust's ownership
   rules at compile time. It ensures that references don't outlive the data they
   point to and prevents data races by allowing either multiple immutable
   references or one mutable reference.

=== System Statistics ===

â†’ Querying database statistics... â€¢ Using find_all() helper - simplified query
API

ğŸ“Š Database Statistics: â€¢ Total conversations: 4 â€¢ Total messages stored: 14 â€¢
All messages indexed for vector search âœ“ â€¢ All messages indexed for text search
âœ“ â€¢ All messages queryable by metadata âœ“

=== Step 8: Dynamic Search Configuration === Each conversation can have its own
search config...

ğŸ’¡ Conversations can store custom search configurations: â€¢ Search type: hybrid,
text, or vector â€¢ Relevance thresholds â€¢ Filter by tags or metadata â€¢
Collection-specific settings â€¢ Per-conversation AI behavior

This enables context-aware search tuned to each conversation's needs!

=== Cleanup === Deleting example collections...

âœ… All done! RAG system demonstrated successfully.

âœ“ Using search results to enhance AI responses (RAG) âœ“ Building a self-improving
knowledge base âœ“ Dynamic search configurations per conversation

ekoDB provides everything needed for AI-powered applications: â€¢ Vector search
(semantic similarity) â€¢ Text search (keyword matching) â€¢ Hybrid search (best of
both) â€¢ AI functions (Chat, Embed) â€¢ Flexible querying and filtering â€¢ All in
one database - no external dependencies!

[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [36mRunning
Python RAG Example...[0m
[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m === ekoDB RAG
Conversation System ===

This example shows how ekoDB can power a self-improving AI system that learns
from its own conversation history.

=== Step 1: Building Conversation History === Storing previous conversations
with embeddings...

â†’ Calling ekoDB embed() helper... â€¢ Using model: text-embedding-3-small â€¢ Text
length: 34 characters â€¢ Behind the scenes: Creating temp Function with Embed
operation âœ“ Generated embedding: 1536 dimensions in 0.409s â€¢ Function
auto-cleaned up by client â†’ Calling ekoDB embed() helper... â€¢ Using model:
text-embedding-3-small â€¢ Text length: 169 characters â€¢ Behind the scenes:
Creating temp Function with Embed operation âœ“ Generated embedding: 1536
dimensions in 0.316s â€¢ Function auto-cleaned up by client â†’ Calling ekoDB
embed() helper... â€¢ Using model: text-embedding-3-small â€¢ Text length: 33
characters â€¢ Behind the scenes: Creating temp Function with Embed operation âœ“
Generated embedding: 1536 dimensions in 0.223s â€¢ Function auto-cleaned up by
client â†’ Calling ekoDB embed() helper... â€¢ Using model: text-embedding-3-small â€¢
Text length: 230 characters â€¢ Behind the scenes: Creating temp Function with
Embed operation âœ“ Generated embedding: 1536 dimensions in 0.267s â€¢ Function
auto-cleaned up by client âœ“ Stored Rust programming conversation (4 messages) â†’
Calling ekoDB embed() helper... â€¢ Using model: text-embedding-3-small â€¢ Text
length: 31 characters â€¢ Behind the scenes: Creating temp Function with Embed
operation âœ“ Generated embedding: 1536 dimensions in 0.204s â€¢ Function
auto-cleaned up by client â†’ Calling ekoDB embed() helper... â€¢ Using model:
text-embedding-3-small â€¢ Text length: 217 characters â€¢ Behind the scenes:
Creating temp Function with Embed operation âœ“ Generated embedding: 1536
dimensions in 0.233s â€¢ Function auto-cleaned up by client â†’ Calling ekoDB
embed() helper... â€¢ Using model: text-embedding-3-small â€¢ Text length: 33
characters â€¢ Behind the scenes: Creating temp Function with Embed operation âœ“
Generated embedding: 1536 dimensions in 0.231s â€¢ Function auto-cleaned up by
client â†’ Calling ekoDB embed() helper... â€¢ Using model: text-embedding-3-small â€¢
Text length: 232 characters â€¢ Behind the scenes: Creating temp Function with
Embed operation âœ“ Generated embedding: 1536 dimensions in 0.311s â€¢ Function
auto-cleaned up by client âœ“ Stored database design conversation (4 messages) â†’
Calling ekoDB embed() helper... â€¢ Using model: text-embedding-3-small â€¢ Text
length: 36 characters â€¢ Behind the scenes: Creating temp Function with Embed
operation âœ“ Generated embedding: 1536 dimensions in 0.300s â€¢ Function
auto-cleaned up by client â†’ Calling ekoDB embed() helper... â€¢ Using model:
text-embedding-3-small â€¢ Text length: 178 characters â€¢ Behind the scenes:
Creating temp Function with Embed operation âœ“ Generated embedding: 1536
dimensions in 0.586s â€¢ Function auto-cleaned up by client â†’ Calling ekoDB
embed() helper... â€¢ Using model: text-embedding-3-small â€¢ Text length: 37
characters â€¢ Behind the scenes: Creating temp Function with Embed operation âœ“
Generated embedding: 1536 dimensions in 0.195s â€¢ Function auto-cleaned up by
client â†’ Calling ekoDB embed() helper... â€¢ Using model: text-embedding-3-small â€¢
Text length: 213 characters â€¢ Behind the scenes: Creating temp Function with
Embed operation âœ“ Generated embedding: 1536 dimensions in 0.307s â€¢ Function
auto-cleaned up by client âœ“ Stored performance optimization conversation (4
messages)

=== Step 2: New User Question with Context Retrieval === User asks: "How do I
write memory-safe high-performance database code?"

=== Step 3: Searching Related Context === Using hybrid search to find relevant
messages from all conversations...

â†’ Generating embedding for user question... â†’ Calling ekoDB embed() helper... â€¢
Using model: text-embedding-3-small â€¢ Text length: 58 characters â€¢ Behind the
scenes: Creating temp Function with Embed operation âœ“ Generated embedding: 1536
dimensions in 0.295s â€¢ Function auto-cleaned up by client

â†’ Executing hybrid_search()... â€¢ Collection: rag_messages â€¢ Query text: "How do
I write memory-safe high-performance database code?" â€¢ Vector dimensions: 1536 â€¢
Limit: 5 results â€¢ Search type: Semantic (vector) + Keyword (text) â€¢ Server
combines both scores for relevance ranking âœ“ Search completed in 0.091s

âœ“ Found 5 related messages across all conversations:

1. [Score: 0.000] From conv_database_design Use NoSQL when you need: flexible
   schemas, horizontal scaling, high write throughput, or when working with
   unstructured data. SQL is better for complex queries, ACID transactions, and
   structured data with well-defined relationships.

2. [Score: 0.000] From conv_database_design What is database normalization?

3. [Score: 0.000] From conv_database_design Database normalization is the
   process of organizing data to reduce redundancy and improve data integrity.
   It involves dividing large tables into smaller ones and defining
   relationships between them using foreign keys.

4. [Score: 0.000] From conv_database_design When should I use NoSQL over SQL?

5. [Score: 0.000] From conv_performance How can I optimize database queries?

=== Step 4: Generating Context-Aware Response === âœ“ AI Response (with context
from 3 conversations):

Answer: Writing memory-safe and high-performance database code involves several
factors:

1. Choose the Right Database: Depending on your use case, either SQL or NoSQL
   might suit you better. For example, if you need to handle unstructured data,
   have high write throughput, or need horizontal scaling, you might want to
   choose NoSQL (context 1). If your data is structured with well-defined
   relationships, or you need ACID transactions or complex queries, SQL might be
   a better choice (context 1).

2. Database Normalization: Normalization can help improve data integrity and
   reduce redundancy. It involves dividing larger tables into smaller ones and
   defining relationships between them using foreign keys (context 3). This can
   help improve performance by reducing the amount of data that needs to be read
   from or written to the database.

3. Optimize Your Queries: You can optimize your database queries by using
   indexes, using the correct query operators, avoiding full table scans,
   limiting the amount of returned data, and properly utilizing joins,
   subqueries, and other advanced SQL features.

4. Memory Management: Depending on the language you're using to interact with
   the database, there might be specific tools or best practices for managing
   memory. For example, in languages like C++, you need to be careful about
   freeing any memory that you allocate. In more managed languages like Java or
   Python, it's important to understand how the garbage collector works and how
   to write code that doesn't unnecessarily hold onto memory.

5. Error Handling: Proper error handling can prevent memory leaks and other
   potential issues. Always ensure that your code can gracefully handle any
   database errors and clean up any resources it was using.

6. Testing and Profiling: Regularly test your code for performance and memory
   usage. Profiling tools can help you identify any bottlenecks or memory leaks.

Remember, the specifics will depend on your particular use case, the database
you're using, and the language you're writing your code in.

=== Step 5: Storing New Conversation === â†’ Calling ekoDB embed() helper... â€¢
Using model: text-embedding-3-small â€¢ Text length: 58 characters â€¢ Behind the
scenes: Creating temp Function with Embed operation âœ“ Generated embedding: 1536
dimensions in 0.388s â€¢ Function auto-cleaned up by client â†’ Calling ekoDB
embed() helper... â€¢ Using model: text-embedding-3-small â€¢ Text length: 2040
characters â€¢ Behind the scenes: Creating temp Function with Embed operation âœ“
Generated embedding: 1536 dimensions in 0.245s â€¢ Function auto-cleaned up by
client âœ“ New conversation stored and indexed for future retrieval

=== Step 6: Cross-Conversation Search === Searching for messages about
'ownership' across ALL conversations...

âœ“ Found 3 messages mentioning ownership:

1. From conv_performance: Rust's ownership system provides zero-cost memory
   management. Use Box for heap allocation, Rc/Arc for shared ownership, and
   avoid cloning large data structures. The compiler optimizes away unnecessary
   allocations.

2. From conv_rust_programming: Rust's key features include: memory safety
   without garbage collection, zero-cost abstractions, ownership system,
   powerful type system, and excellent concurrency support.

3. From conv_rust_programming: The borrow checker enforces Rust's ownership
   rules at compile time. It ensures that references don't outlive the data they
   point to and prevents data races by allowing either multiple immutable
   references or one mutable reference.

=== System Statistics === Total conversations: 4 Total messages stored: 14 All
messages are indexed for vector search âœ“ All messages are indexed for text
search âœ“ All messages are queryable by metadata âœ“

=== Step 8: Dynamic Search Configuration === Each conversation can have its own
search config...

ğŸ’¡ Conversations can store custom search configurations: â€¢ Search type: hybrid,
text, or vector â€¢ Relevance thresholds â€¢ Filter by tags or metadata â€¢
Collection-specific settings â€¢ Per-conversation AI behavior

This enables context-aware search tuned to each conversation's needs!

=== Cleanup === âœ“ Cleanup complete

=== ğŸ“š Summary: What This Example Showed ===

ğŸ”§ ekoDB Native Capabilities Used: âœ“ Functions with Embed operation (AI
integration) âœ“ Hybrid Search (text + vector combined) âœ“ Text Search (full-text
with stemming) âœ“ Automatic embedding generation âœ“ Cross-collection queries

ğŸš€ New Client Helper Methods: â€¢ client.embed(text, model) - Generate embeddings
â€¢ client.hybrid_search() - Semantic + keyword search â€¢ client.text_search() -
Full-text search â€¢ client.find_all() - Query all documents

ğŸ’¡ Key Takeaways:

1. ekoDB handles AI Functions natively - no external services needed
2. One-line embedding generation with auto-cleanup
3. Hybrid search combines semantic understanding + keyword matching
4. Perfect for RAG: store, search, and retrieve context
5. All AI capabilities accessible through simple client methods

ğŸ¯ Build production RAG systems with ekoDB! â†’ Set OPENAI_API_KEY in your ekoDB
server environment â†’ Use these client helpers to make AI integration simple â†’
Scale to millions of documents with native indexing

[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [36mRunning
TypeScript RAG Example...[0m
[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m === ekoDB RAG
Conversation System ===

This example shows how ekoDB can power a self-improving AI system that learns
from its own conversation history.

=== Step 1: Building Conversation History === Storing previous conversations
with embeddings...

â†’ Calling ekoDB embed() helper... â€¢ Using model: text-embedding-3-small â€¢ Text
length: 34 characters â€¢ Behind the scenes: Creating temp Function with Embed
operation âœ“ Generated embedding: 1536 dimensions in 0.311s â€¢ Function
auto-cleaned up by client â†’ Calling ekoDB embed() helper... â€¢ Using model:
text-embedding-3-small â€¢ Text length: 169 characters â€¢ Behind the scenes:
Creating temp Function with Embed operation âœ“ Generated embedding: 1536
dimensions in 0.303s â€¢ Function auto-cleaned up by client â†’ Calling ekoDB
embed() helper... â€¢ Using model: text-embedding-3-small â€¢ Text length: 33
characters â€¢ Behind the scenes: Creating temp Function with Embed operation âœ“
Generated embedding: 1536 dimensions in 0.277s â€¢ Function auto-cleaned up by
client â†’ Calling ekoDB embed() helper... â€¢ Using model: text-embedding-3-small â€¢
Text length: 230 characters â€¢ Behind the scenes: Creating temp Function with
Embed operation âœ“ Generated embedding: 1536 dimensions in 0.263s â€¢ Function
auto-cleaned up by client âœ“ Stored Rust programming conversation (4 messages) â†’
Calling ekoDB embed() helper... â€¢ Using model: text-embedding-3-small â€¢ Text
length: 31 characters â€¢ Behind the scenes: Creating temp Function with Embed
operation âœ“ Generated embedding: 1536 dimensions in 0.264s â€¢ Function
auto-cleaned up by client â†’ Calling ekoDB embed() helper... â€¢ Using model:
text-embedding-3-small â€¢ Text length: 217 characters â€¢ Behind the scenes:
Creating temp Function with Embed operation âœ“ Generated embedding: 1536
dimensions in 0.388s â€¢ Function auto-cleaned up by client â†’ Calling ekoDB
embed() helper... â€¢ Using model: text-embedding-3-small â€¢ Text length: 33
characters â€¢ Behind the scenes: Creating temp Function with Embed operation âœ“
Generated embedding: 1536 dimensions in 0.199s â€¢ Function auto-cleaned up by
client â†’ Calling ekoDB embed() helper... â€¢ Using model: text-embedding-3-small â€¢
Text length: 232 characters â€¢ Behind the scenes: Creating temp Function with
Embed operation âœ“ Generated embedding: 1536 dimensions in 0.356s â€¢ Function
auto-cleaned up by client âœ“ Stored database design conversation (4 messages) â†’
Calling ekoDB embed() helper... â€¢ Using model: text-embedding-3-small â€¢ Text
length: 36 characters â€¢ Behind the scenes: Creating temp Function with Embed
operation âœ“ Generated embedding: 1536 dimensions in 0.214s â€¢ Function
auto-cleaned up by client â†’ Calling ekoDB embed() helper... â€¢ Using model:
text-embedding-3-small â€¢ Text length: 178 characters â€¢ Behind the scenes:
Creating temp Function with Embed operation âœ“ Generated embedding: 1536
dimensions in 0.326s â€¢ Function auto-cleaned up by client â†’ Calling ekoDB
embed() helper... â€¢ Using model: text-embedding-3-small â€¢ Text length: 37
characters â€¢ Behind the scenes: Creating temp Function with Embed operation âœ“
Generated embedding: 1536 dimensions in 0.309s â€¢ Function auto-cleaned up by
client â†’ Calling ekoDB embed() helper... â€¢ Using model: text-embedding-3-small â€¢
Text length: 213 characters â€¢ Behind the scenes: Creating temp Function with
Embed operation âœ“ Generated embedding: 1536 dimensions in 0.410s â€¢ Function
auto-cleaned up by client âœ“ Stored performance optimization conversation (4
messages)

=== Step 2: New User Question with Context Retrieval === User asks: "How do I
write memory-safe high-performance database code?"

=== Step 3: Searching Related Context === Using hybrid search to find relevant
messages from all conversations...

â†’ Generating embedding for user question... â†’ Calling ekoDB embed() helper... â€¢
Using model: text-embedding-3-small â€¢ Text length: 58 characters â€¢ Behind the
scenes: Creating temp Function with Embed operation âœ“ Generated embedding: 1536
dimensions in 0.286s â€¢ Function auto-cleaned up by client

â†’ Executing hybridSearch()... â€¢ Collection: rag_messages â€¢ Query text: "How do I
write memory-safe high-performance database code?" â€¢ Vector dimensions: 1536 â€¢
Limit: 5 results â€¢ Search type: Semantic (vector) + Keyword (text) â€¢ Server
combines both scores for relevance ranking âœ“ Search completed in 0.088s âœ“ Found
5 related messages across all conversations:

1. [Score: 0.000] From conv_performance How can I optimize database queries?

2. [Score: 0.000] From conv_database_design Use NoSQL when you need: flexible
   schemas, horizontal scaling, high write throughput, or when working with
   unstructured data. SQL is better for complex queries, ACID transactions, and
   structured data with well-defined relationships.

3. [Score: 0.000] From conv_database_design When should I use NoSQL over SQL?

4. [Score: 0.000] From conv_database_design What is database normalization?

5. [Score: 0.000] From conv_database_design Database normalization is the
   process of organizing data to reduce redundancy and improve data integrity.
   It involves dividing large tables into smaller ones and defining
   relationships between them using foreign keys.

=== Step 4: Generating Context-Aware Response === âœ“ AI Response (with context
from 3 conversations):

Writing memory-safe, high-performance database code involves a combination of
efficient memory management, well-structured queries, and appropriate database
choice. Here are a few strategies:

1. Database Selection: Choose the right database system based on your needs. As
   mentioned in Context 2, NoSQL databases are suitable for flexible schemas,
   horizontal scaling, high write throughput, and unstructured data. SQL
   databases are better for complex queries, ACID transactions, and structured
   data with well-defined relationships.

2. Query Optimization: You can optimize database queries by using indexes,
   avoiding full table scans, limiting the result set, using joins wisely, and
   avoiding N+1 queries. Also, consider using stored procedures or prepared
   statements to reuse code and minimize database hits.

3. Normalization: Database normalization, as described in Context 5, helps
   improve data integrity and reduces redundancy. This process organizes data by
   dividing large tables into smaller ones and establishing relationships
   between them using foreign keys. Normalization can lead to performance
   improvements due to decreased redundancy.

4. Memory Management: Be mindful of the memory usage in your code. Avoid storing
   large datasets in memory all at once, and instead process data in chunks if
   possible. In addition, ensure that any resources are properly released after
   use to prevent memory leaks.

5. Concurrency: Implement concurrency control mechanisms to handle multiple
   simultaneous operations. This can help to maximize throughput and minimize
   contention for resources.

6. Caching: Use caching techniques to store frequently accessed data in memory,
   reducing the need for expensive database calls.

By combining these strategies, you can write code that is both memory-safe and
high-performance.

=== Step 5: Storing New Conversation === â†’ Calling ekoDB embed() helper... â€¢
Using model: text-embedding-3-small â€¢ Text length: 58 characters â€¢ Behind the
scenes: Creating temp Function with Embed operation âœ“ Generated embedding: 1536
dimensions in 0.295s â€¢ Function auto-cleaned up by client â†’ Calling ekoDB
embed() helper... â€¢ Using model: text-embedding-3-small â€¢ Text length: 1816
characters â€¢ Behind the scenes: Creating temp Function with Embed operation âœ“
Generated embedding: 1536 dimensions in 0.270s â€¢ Function auto-cleaned up by
client âœ“ New conversation stored and indexed for future retrieval

=== Step 6: Cross-Conversation Search === Searching for messages about
'ownership' across ALL conversations...

â†’ Executing textSearch()... â€¢ Collection: rag_messages â€¢ Query: "ownership
system" â€¢ Limit: 3 results â€¢ Search method: Full-text with fuzzy matching &
stemming â€¢ No vector embeddings needed - pure keyword search âœ“ Text search
completed in 0.052s âœ“ Found 3 messages mentioning ownership:

1. From conv_performance: Rust's ownership system provides zero-cost memory
   management. Use Box for heap allocation, Rc/Arc for shared ownership, and
   avoid cloning large data structures. The compiler optimizes away unnecessary
   allocations.

2. From conv_rust_programming: Rust's key features include: memory safety
   without garbage collection, zero-cost abstractions, ownership system,
   powerful type system, and excellent concurrency support.

3. From conv_rust_programming: The borrow checker enforces Rust's ownership
   rules at compile time. It ensures that references don't outlive the data they
   point to and prevents data races by allowing either multiple immutable
   references or one mutable reference.

=== System Statistics ===

â†’ Querying database statistics... â€¢ Using findAllWithLimit() helper - simplified
query API

ğŸ“Š Database Statistics: â€¢ Total conversations: 4 â€¢ Total messages stored: 14 â€¢
All messages indexed for vector search âœ“ â€¢ All messages indexed for text search
âœ“ â€¢ All messages queryable by metadata âœ“

=== Step 8: Dynamic Search Configuration === Each conversation can have its own
search config...

ğŸ’¡ Conversations can store custom search configurations: â€¢ Search type: hybrid,
text, or vector â€¢ Relevance thresholds â€¢ Filter by tags or metadata â€¢
Collection-specific settings â€¢ Per-conversation AI behavior

This enables context-aware search tuned to each conversation's needs!

=== Cleanup === âœ“ Cleanup complete

=== ğŸ“š Summary: What This Example Showed ===

ğŸ”§ ekoDB Native Capabilities Used: âœ“ Functions with Embed operation (AI
integration) âœ“ Hybrid Search (text + vector combined) âœ“ Text Search (full-text
with stemming) âœ“ Automatic embedding generation âœ“ Cross-collection queries

ğŸš€ New Client Helper Methods: â€¢ client.embed(text, model) - Generate embeddings
â€¢ client.hybridSearch() - Semantic + keyword search â€¢ client.textSearch() -
Full-text search â€¢ client.findAllWithLimit() - Query all documents

ğŸ’¡ Key Takeaways:

1. ekoDB handles AI Functions natively - no external services needed
2. One-line embedding generation with auto-cleanup
3. Hybrid search combines semantic understanding + keyword matching
4. Perfect for RAG: store, search, and retrieve context
5. All AI capabilities accessible through simple client methods

ğŸ¯ Build production RAG systems with ekoDB! â†’ Set OPENAI_API_KEY in your ekoDB
server environment â†’ Use these client helpers to make AI integration simple â†’
Scale to millions of documents with native indexing

[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [36mRunning Go
RAG Example...[0m
[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m === ekoDB RAG
Conversation System ===

This example shows how ekoDB can power a self-improving AI system that learns
from its own conversation history.

=== Step 1: Building Conversation History === Storing previous conversations
with embeddings...

â†’ Calling ekoDB Embed() helper... â€¢ Using model: text-embedding-3-small â€¢ Text
length: 34 characters â€¢ Behind the scenes: Creating temp Function with Embed
operation âœ“ Generated embedding: 1536 dimensions in 0.247s â€¢ Function
auto-cleaned up by client â†’ Calling ekoDB Embed() helper... â€¢ Using model:
text-embedding-3-small â€¢ Text length: 169 characters â€¢ Behind the scenes:
Creating temp Function with Embed operation âœ“ Generated embedding: 1536
dimensions in 0.456s â€¢ Function auto-cleaned up by client â†’ Calling ekoDB
Embed() helper... â€¢ Using model: text-embedding-3-small â€¢ Text length: 33
characters â€¢ Behind the scenes: Creating temp Function with Embed operation âœ“
Generated embedding: 1536 dimensions in 0.240s â€¢ Function auto-cleaned up by
client â†’ Calling ekoDB Embed() helper... â€¢ Using model: text-embedding-3-small â€¢
Text length: 230 characters â€¢ Behind the scenes: Creating temp Function with
Embed operation âœ“ Generated embedding: 1536 dimensions in 0.266s â€¢ Function
auto-cleaned up by client âœ“ Stored Rust programming conversation (4 messages) â†’
Calling ekoDB Embed() helper... â€¢ Using model: text-embedding-3-small â€¢ Text
length: 31 characters â€¢ Behind the scenes: Creating temp Function with Embed
operation âœ“ Generated embedding: 1536 dimensions in 0.256s â€¢ Function
auto-cleaned up by client â†’ Calling ekoDB Embed() helper... â€¢ Using model:
text-embedding-3-small â€¢ Text length: 217 characters â€¢ Behind the scenes:
Creating temp Function with Embed operation âœ“ Generated embedding: 1536
dimensions in 0.278s â€¢ Function auto-cleaned up by client â†’ Calling ekoDB
Embed() helper... â€¢ Using model: text-embedding-3-small â€¢ Text length: 33
characters â€¢ Behind the scenes: Creating temp Function with Embed operation âœ“
Generated embedding: 1536 dimensions in 0.278s â€¢ Function auto-cleaned up by
client â†’ Calling ekoDB Embed() helper... â€¢ Using model: text-embedding-3-small â€¢
Text length: 232 characters â€¢ Behind the scenes: Creating temp Function with
Embed operation âœ“ Generated embedding: 1536 dimensions in 0.289s â€¢ Function
auto-cleaned up by client âœ“ Stored database design conversation (4 messages) â†’
Calling ekoDB Embed() helper... â€¢ Using model: text-embedding-3-small â€¢ Text
length: 36 characters â€¢ Behind the scenes: Creating temp Function with Embed
operation âœ“ Generated embedding: 1536 dimensions in 0.250s â€¢ Function
auto-cleaned up by client â†’ Calling ekoDB Embed() helper... â€¢ Using model:
text-embedding-3-small â€¢ Text length: 178 characters â€¢ Behind the scenes:
Creating temp Function with Embed operation âœ“ Generated embedding: 1536
dimensions in 0.329s â€¢ Function auto-cleaned up by client â†’ Calling ekoDB
Embed() helper... â€¢ Using model: text-embedding-3-small â€¢ Text length: 37
characters â€¢ Behind the scenes: Creating temp Function with Embed operation âœ“
Generated embedding: 1536 dimensions in 0.289s â€¢ Function auto-cleaned up by
client â†’ Calling ekoDB Embed() helper... â€¢ Using model: text-embedding-3-small â€¢
Text length: 213 characters â€¢ Behind the scenes: Creating temp Function with
Embed operation âœ“ Generated embedding: 1536 dimensions in 0.281s â€¢ Function
auto-cleaned up by client âœ“ Stored performance optimization conversation (4
messages)

=== Step 2: New User Question with Context Retrieval === User asks: "How do I
write memory-safe high-performance database code?"

=== Step 3: Searching Related Context === Using hybrid search to find relevant
messages from all conversations...

â†’ Generating embedding for user question... â†’ Calling ekoDB Embed() helper... â€¢
Using model: text-embedding-3-small â€¢ Text length: 58 characters â€¢ Behind the
scenes: Creating temp Function with Embed operation âœ“ Generated embedding: 1536
dimensions in 0.482s â€¢ Function auto-cleaned up by client

â†’ Executing HybridSearch()... â€¢ Collection: rag_messages â€¢ Query text: "How do I
write memory-safe high-performance database code?" â€¢ Vector dimensions: 1536 â€¢
Limit: 5 results â€¢ Search type: Semantic (vector) + Keyword (text) â€¢ Server
combines both scores for relevance ranking âœ“ Search completed in 0.082s

âœ“ Found 5 related messages across all conversations:

1. [Score: 0.000] From conv_database_design Database normalization is the
   process of organizing data to reduce redundancy and improve data integrity.
   It involves dividing large tables into smaller ones and defining
   relationships between them using foreign keys.

2. [Score: 0.000] From conv_database_design Use NoSQL when you need: flexible
   schemas, horizontal scaling, high write throughput, or when working with
   unstructured data. SQL is better for complex queries, ACID transactions, and
   structured data with well-defined relationships.

3. [Score: 0.000] From conv_database_design What is database normalization?

4. [Score: 0.000] From conv_database_design When should I use NoSQL over SQL?

5. [Score: 0.000] From conv_performance How can I optimize database queries?

=== Step 4: Generating Context-Aware Response === âœ“ AI Response (with context
from 3 conversations):

Answer: Writing memory-safe, high-performance database code involves a
combination of good practices including efficient database design, effective use
of SQL or NoSQL depending on your needs, as well as optimizing your database
queries.

1. Efficient Database Design: This is where concepts like 'Database
   Normalization' come into play. Normalization helps in reducing data
   redundancy and improving data integrity by dividing large tables into smaller
   ones and defining relationships between them using foreign keys.

2. Choice of SQL or NoSQL: Depending on your needs, you may choose either SQL or
   NoSQL databases. Use NoSQL for flexible schemas, horizontal scaling, high
   write throughput, or when working with unstructured data. On the other hand,
   SQL is better for complex queries, ACID transactions, and structured data
   with well-defined relationships.

3. Optimizing Database Queries: To optimize your database queries, ensure that
   you're only retrieving the data you need (limit the use of SELECT \*), make
   good use of indexing to speed up retrieval of data, avoid using heavy
   operations like HAVING and UNION where you can use WHERE and JOIN
   respectively. Also, avoid N+1 queries where you load child entities one by
   one, instead, use join queries to load them in a single trip to your
   database.

4. Memory Management: In terms of memory safety, ensure that you close all
   database connections after operations to free up resources. Also, use
   parameterized queries or prepared statements to guard against SQL injection,
   which can lead to unauthorized use of memory.

5. Regular Monitoring and Maintenance: Regularly monitor the performance of your
   database. Use tools that can help identify slow queries, analyze database
   workload, and suggest indexes. Regular database maintenance like updating
   statistics, rebuilding indexes, and removing old data also helps in
   maintaining high performance.

Remember, what works best can depend on the specific database system you're
using, and the specific use case of your application.

=== Step 5: Storing New Conversation === â†’ Calling ekoDB Embed() helper... â€¢
Using model: text-embedding-3-small â€¢ Text length: 58 characters â€¢ Behind the
scenes: Creating temp Function with Embed operation âœ“ Generated embedding: 1536
dimensions in 0.332s â€¢ Function auto-cleaned up by client â†’ Calling ekoDB
Embed() helper... â€¢ Using model: text-embedding-3-small â€¢ Text length: 2030
characters â€¢ Behind the scenes: Creating temp Function with Embed operation âœ“
Generated embedding: 1536 dimensions in 0.229s â€¢ Function auto-cleaned up by
client âœ“ New conversation stored and indexed for future retrieval

=== Step 6: Cross-Conversation Search === Searching for messages about
'ownership' across ALL conversations...

â†’ Executing TextSearch()... â€¢ Collection: rag_messages â€¢ Query: "ownership
system" â€¢ Limit: 3 results â€¢ Search method: Full-text with fuzzy matching &
stemming â€¢ No vector embeddings needed - pure keyword search âœ“ Text search
completed in 0.054s

âœ“ Found 3 messages mentioning ownership:

1. From conv_rust_programming: Rust's key features include: memory safety
   without garbage collection, zero-cost abstractions, ownership system,
   powerful type system, and excellent concurrency support.

2. From conv_performance: Rust's ownership system provides zero-cost memory
   management. Use Box for heap allocation, Rc/Arc for shared ownership, and
   avoid cloning large data structures. The compiler optimizes away unnecessary
   allocations.

3. From conv_new_question: Answer: Writing memory-safe, high-performance
   database code involves a combination of good practices including efficient
   database design, effective use of SQL or NoSQL depending on your needs, as
   well as optimizing your database queries.

4. Efficient Database Design: This is where concepts like 'Database
   Normalization' come into play. Normalization helps in reducing data
   redundancy and improving data integrity by dividing large tables into smaller
   ones and defining relationships between them using foreign keys.

5. Choice of SQL or NoSQL: Depending on your needs, you may choose either SQL or
   NoSQL databases. Use NoSQL for flexible schemas, horizontal scaling, high
   write throughput, or when working with unstructured data. On the other hand,
   SQL is better for complex queries, ACID transactions, and structured data
   with well-defined relationships.

6. Optimizing Database Queries: To optimize your database queries, ensure that
   you're only retrieving the data you need (limit the use of SELECT \*), make
   good use of indexing to speed up retrieval of data, avoid using heavy
   operations like HAVING and UNION where you can use WHERE and JOIN
   respectively. Also, avoid N+1 queries where you load child entities one by
   one, instead, use join queries to load them in a single trip to your
   database.

7. Memory Management: In terms of memory safety, ensure that you close all
   database connections after operations to free up resources. Also, use
   parameterized queries or prepared statements to guard against SQL injection,
   which can lead to unauthorized use of memory.

8. Regular Monitoring and Maintenance: Regularly monitor the performance of your
   database. Use tools that can help identify slow queries, analyze database
   workload, and suggest indexes. Regular database maintenance like updating
   statistics, rebuilding indexes, and removing old data also helps in
   maintaining high performance.

Remember, what works best can depend on the specific database system you're
using, and the specific use case of your application.

=== System Statistics ===

â†’ Querying database statistics... â€¢ Using FindAll() helper - simplified query
API

ğŸ“Š Database Statistics: â€¢ Total conversations: 4 â€¢ Total messages stored: 14 â€¢
All messages indexed for vector search âœ“ â€¢ All messages indexed for text search
âœ“ â€¢ All messages queryable by metadata âœ“

=== Step 8: Dynamic Search Configuration === Each conversation can have its own
search config...

ğŸ’¡ Conversations can store custom search configurations: â€¢ Search type: hybrid,
text, or vector â€¢ Relevance thresholds â€¢ Filter by tags or metadata â€¢
Collection-specific settings â€¢ Per-conversation AI behavior

This enables context-aware search tuned to each conversation's needs!

=== Cleanup === âœ“ Cleanup complete

=== ğŸ“š Summary: What This Example Showed ===

ğŸ”§ ekoDB Native Capabilities Used: âœ“ Functions with Embed operation (AI
integration) âœ“ Hybrid Search (text + vector combined) âœ“ Text Search (full-text
with stemming) âœ“ Automatic embedding generation âœ“ Cross-collection queries

ğŸš€ New Client Helper Methods: â€¢ client.Embed(text, model) - Generate embeddings
â€¢ client.HybridSearch() - Semantic + keyword search â€¢ client.TextSearch() -
Full-text search â€¢ client.FindAll() - Query all documents

ğŸ’¡ Key Takeaways:

1. ekoDB handles AI Functions natively - no external services needed
2. One-line embedding generation with auto-cleanup
3. Hybrid search combines semantic understanding + keyword matching
4. Perfect for RAG: store, search, and retrieve context
5. All AI capabilities accessible through simple client methods

ğŸ¯ Build production RAG systems with ekoDB! â†’ Set OPENAI_API_KEY in your ekoDB
server environment â†’ Use these client helpers to make AI integration simple â†’
Scale to millions of documents with native indexing

[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [36mRunning
Kotlin RAG Example...[0m
[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m === ekoDB RAG
Conversation System ===

This example shows how ekoDB can power a self-improving AI system that learns
from its own conversation history.

SLF4J(W): No SLF4J providers were found. SLF4J(W): Defaulting to no-operation
(NOP) logger implementation SLF4J(W): See
https://www.slf4j.org/codes.html#noProviders for further details. === Step 1:
Building Conversation History === Storing previous conversations with
embeddings...

â†’ Calling ekoDB embed() helper... â€¢ Using model: text-embedding-3-small â€¢ Text
length: 34 characters â€¢ Behind the scenes: Creating temp Function with Embed
operation âœ“ Generated embedding: 1536 dimensions in 0.346s â€¢ Function
auto-cleaned up by client â†’ Calling ekoDB embed() helper... â€¢ Using model:
text-embedding-3-small â€¢ Text length: 169 characters â€¢ Behind the scenes:
Creating temp Function with Embed operation âœ“ Generated embedding: 1536
dimensions in 0.218s â€¢ Function auto-cleaned up by client â†’ Calling ekoDB
embed() helper... â€¢ Using model: text-embedding-3-small â€¢ Text length: 33
characters â€¢ Behind the scenes: Creating temp Function with Embed operation âœ“
Generated embedding: 1536 dimensions in 0.343s â€¢ Function auto-cleaned up by
client â†’ Calling ekoDB embed() helper... â€¢ Using model: text-embedding-3-small â€¢
Text length: 230 characters â€¢ Behind the scenes: Creating temp Function with
Embed operation âœ“ Generated embedding: 1536 dimensions in 0.299s â€¢ Function
auto-cleaned up by client âœ“ Stored Rust programming conversation (4 messages) â†’
Calling ekoDB embed() helper... â€¢ Using model: text-embedding-3-small â€¢ Text
length: 31 characters â€¢ Behind the scenes: Creating temp Function with Embed
operation âœ“ Generated embedding: 1536 dimensions in 0.327s â€¢ Function
auto-cleaned up by client â†’ Calling ekoDB embed() helper... â€¢ Using model:
text-embedding-3-small â€¢ Text length: 217 characters â€¢ Behind the scenes:
Creating temp Function with Embed operation âœ“ Generated embedding: 1536
dimensions in 0.256s â€¢ Function auto-cleaned up by client â†’ Calling ekoDB
embed() helper... â€¢ Using model: text-embedding-3-small â€¢ Text length: 33
characters â€¢ Behind the scenes: Creating temp Function with Embed operation âœ“
Generated embedding: 1536 dimensions in 0.264s â€¢ Function auto-cleaned up by
client â†’ Calling ekoDB embed() helper... â€¢ Using model: text-embedding-3-small â€¢
Text length: 232 characters â€¢ Behind the scenes: Creating temp Function with
Embed operation âœ“ Generated embedding: 1536 dimensions in 0.217s â€¢ Function
auto-cleaned up by client âœ“ Stored database design conversation (4 messages) â†’
Calling ekoDB embed() helper... â€¢ Using model: text-embedding-3-small â€¢ Text
length: 36 characters â€¢ Behind the scenes: Creating temp Function with Embed
operation âœ“ Generated embedding: 1536 dimensions in 0.226s â€¢ Function
auto-cleaned up by client â†’ Calling ekoDB embed() helper... â€¢ Using model:
text-embedding-3-small â€¢ Text length: 178 characters â€¢ Behind the scenes:
Creating temp Function with Embed operation âœ“ Generated embedding: 1536
dimensions in 0.252s â€¢ Function auto-cleaned up by client â†’ Calling ekoDB
embed() helper... â€¢ Using model: text-embedding-3-small â€¢ Text length: 37
characters â€¢ Behind the scenes: Creating temp Function with Embed operation âœ“
Generated embedding: 1536 dimensions in 0.559s â€¢ Function auto-cleaned up by
client â†’ Calling ekoDB embed() helper... â€¢ Using model: text-embedding-3-small â€¢
Text length: 213 characters â€¢ Behind the scenes: Creating temp Function with
Embed operation âœ“ Generated embedding: 1536 dimensions in 0.255s â€¢ Function
auto-cleaned up by client âœ“ Stored performance optimization conversation (4
messages)

=== Step 2: New User Question with Context Retrieval === User asks: "How do I
write memory-safe high-performance database code?"

=== Step 3: Searching Related Context === Using hybrid search to find relevant
messages from all conversations...

â†’ Generating embedding for user question... â†’ Calling ekoDB embed() helper... â€¢
Using model: text-embedding-3-small â€¢ Text length: 58 characters â€¢ Behind the
scenes: Creating temp Function with Embed operation âœ“ Generated embedding: 1536
dimensions in 0.221s â€¢ Function auto-cleaned up by client

â†’ Executing hybridSearch()... â€¢ Collection: rag_messages â€¢ Query text: "How do I
write memory-safe high-performance database code?" â€¢ Vector dimensions: 1536 â€¢
Limit: 5 results â€¢ Search type: Semantic (vector) + Keyword (text) â€¢ Server
combines both scores for relevance ranking âœ“ Search completed in 0.08s

âœ“ Found 5 related messages across all conversations:

1. [Score: 0.000] From conv_database_design Database normalization is the
   process of organizing data to reduce redundancy and improve data integrity.
   It involves dividing large tables into smaller ones and defining
   relationships between them using foreign keys.

2. [Score: 0.000] From conv_database_design Use NoSQL when you need: flexible
   schemas, horizontal scaling, high write throughput, or when working with
   unstructured data. SQL is better for complex queries, ACID transactions, and
   structured data with well-defined relationships.

3. [Score: 0.000] From conv_database_design What is database normalization?

4. [Score: 0.000] From conv_database_design When should I use NoSQL over SQL?

5. [Score: 0.000] From conv_performance How can I optimize database queries?

=== Step 4: Generating Context-Aware Response === âœ“ Context prepared from search
results âœ“ AI would use this context to generate comprehensive response

=== Step 5: Storing New Conversation === â†’ Calling ekoDB embed() helper... â€¢
Using model: text-embedding-3-small â€¢ Text length: 58 characters â€¢ Behind the
scenes: Creating temp Function with Embed operation âœ“ Generated embedding: 1536
dimensions in 0.23s â€¢ Function auto-cleaned up by client âœ“ New conversation
stored and indexed for future retrieval

=== Step 6: Cross-Conversation Search === Searching for messages about
'ownership' across ALL conversations...

â†’ Executing textSearch()... â€¢ Collection: rag_messages â€¢ Query: "ownership
system" â€¢ Limit: 3 results â€¢ Search method: Full-text with fuzzy matching &
stemming â€¢ No vector embeddings needed - pure keyword search âœ“ Text search
completed in 0.04s

âœ“ Found 3 messages mentioning ownership:

1. From conv_rust_programming: Rust's key features include: memory safety
   without garbage collection, zero-cost abstractions, ownership system,
   powerful type system, and excellent concurrency support.

2. From conv_performance: Rust's ownership system provides zero-cost memory
   management. Use Box for heap allocation, Rc/Arc for shared ownership, and
   avoid cloning large data structures. The compiler optimizes away unnecessary
   allocations.

3. From conv_rust_programming: The borrow checker enforces Rust's ownership
   rules at compile time. It ensures that references don't outlive the data they
   point to and prevents data races by allowing either multiple immutable
   references or one mutable reference.

=== System Statistics ===

â†’ Querying database statistics... â€¢ Using findAllWithLimit() helper - simplified
query API

ğŸ“Š Database Statistics: â€¢ Total conversations: 4 â€¢ Total messages stored: 13 â€¢
All messages indexed for vector search âœ“ â€¢ All messages indexed for text search
âœ“ â€¢ All messages queryable by metadata âœ“

=== Step 8: Dynamic Search Configuration === Each conversation can have its own
search config...

ğŸ’¡ Conversations can store custom search configurations: â€¢ Search type: hybrid,
text, or vector â€¢ Relevance thresholds â€¢ Filter by tags or metadata â€¢
Collection-specific settings â€¢ Per-conversation AI behavior

This enables context-aware search tuned to each conversation's needs!

=== Cleanup === âœ“ Cleanup complete

=== ğŸ“š Summary: What This Example Showed ===

ğŸ”§ ekoDB Native Capabilities Used: âœ“ Functions with Embed operation (AI
integration) âœ“ Hybrid Search (text + vector combined) âœ“ Text Search (full-text
with stemming) âœ“ Automatic embedding generation âœ“ Cross-collection queries

ğŸš€ New Client Helper Methods: â€¢ client.embed(text, model) - Generate embeddings
â€¢ client.hybridSearch() - Semantic + keyword search â€¢ client.textSearch() -
Full-text search â€¢ client.findAllWithLimit() - Query all documents

ğŸ’¡ Key Takeaways:

1. ekoDB handles AI Functions natively - no external services needed
2. One-line embedding generation with auto-cleanup
3. Hybrid search combines semantic understanding + keyword matching
4. Perfect for RAG: store, search, and retrieve context
5. All AI capabilities accessible through simple client methods

ğŸ¯ Build production RAG systems with ekoDB! â†’ Set OPENAI_API_KEY in your ekoDB
server environment â†’ Use these client helpers to make AI integration simple â†’
Scale to millions of documents with native indexing

[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m âœ… [32mRAG
Examples Complete![0m
[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m

[32mWhat you just saw across 5 languages:[0m âœ“ Embeddings generated via ekoDB
Functions âœ“ Hybrid search (semantic + keyword) âœ“ Text search with stemming âœ“
Cross-conversation context retrieval âœ“ Simple client helpers wrapping powerful
AI

[36mMission: AI for All ğŸš€[0m - Making RAG accessible to everyone!
