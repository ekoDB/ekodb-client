make test-examples-rag

ğŸ¤– [36mRAG Conversation System Examples[0m
[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m

[33mPrerequisites:[0m
  1. ekoDB server running (make run in ekodb/)
  2. OPENAI_API_KEY set in server environment
  3. API_BASE_URL and API_BASE_KEY exported in your shell

[36mBuilding Rust client library...[0m
âœ“ Rust client built

[36mBuilding Python client bindings...[0m
ğŸ¹ Building a mixed python/rust project
ğŸ”— Found pyo3 bindings with abi3 support
ğŸ Not using a specific python interpreter
ğŸ’» Using `MACOSX_DEPLOYMENT_TARGET=11.0` for aarch64-apple-darwin by default
ğŸ“¦ Built wheel for abi3 Python â‰¥ 3.8 to /Users/tek/Development/ekoDB/ekodb-client/ekodb-client-py/target/wheels/ekodb_client-0.7.1-cp38-abi3-macosx_11_0_arm64.whl
âœ“ Python client built and installed

[36mBuilding TypeScript client library...[0m
âœ“ TypeScript client built

[36mInstalling TypeScript client in examples...[0m
âœ“ TypeScript client installed

[36mBuilding TypeScript example...[0m
âœ“ TypeScript example built

[36mBuilding Go client library...[0m
âœ“ Go client built

[36mBuilding Go RAG example...[0m
âœ“ Go example built

[36mBuilding Kotlin client library...[0m
âœ“ Kotlin client built

[36mBuilding Kotlin RAG example...[0m
âœ“ Kotlin example built

[36mRunning Rust RAG Example...[0m
[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m
=== ekoDB RAG Conversation System ===

This example shows how ekoDB can power a self-improving AI system
that learns from its own conversation history.

=== Step 1: Building Conversation History ===
Storing previous conversations with embeddings...

  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 34 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 208.77725ms
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 169 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 254.343125ms
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 33 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 244.90075ms
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 230 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 240.68825ms
    â€¢ Function auto-cleaned up by client
âœ“ Stored Rust programming conversation (4 messages)
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 31 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 234.642042ms
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 217 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 943.453459ms
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 33 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 609.7295ms
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 232 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 508.168167ms
    â€¢ Function auto-cleaned up by client
âœ“ Stored database design conversation (4 messages)
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 36 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 1.028662458s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 178 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 348.257667ms
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 37 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 596.014167ms
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 213 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 286.876292ms
    â€¢ Function auto-cleaned up by client
âœ“ Stored performance optimization conversation (4 messages)

=== Step 2: New User Question with Context Retrieval ===
User asks: "How do I write memory-safe high-performance database code?"

=== Step 3: Searching Related Context ===
Using hybrid search to find relevant messages from all conversations...


â†’ Generating embedding for user question...
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 58 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 258.7915ms
    â€¢ Function auto-cleaned up by client

â†’ Executing hybrid_search()...
  â€¢ Collection: rag_messages
  â€¢ Query text: "How do I write memory-safe high-performance database code?"
  â€¢ Vector dimensions: 1536
  â€¢ Limit: 5 results
  â€¢ Search type: Semantic (vector) + Keyword (text)
  â€¢ Server combines both scores for relevance ranking
  âœ“ Search completed in 95.57025ms

âœ“ Found 5 related messages across all conversations:
  1. From conv_database_design
     Use NoSQL when you need: flexible schemas, horizontal scaling, high write throughput, or when working with unstructured data. SQL is better for complex queries, ACID transactions, and structured data with well-defined relationships.

  2. From conv_database_design
     What is database normalization?

  3. From conv_database_design
     Database normalization is the process of organizing data to reduce redundancy and improve data integrity. It involves dividing large tables into smaller ones and defining relationships between them using foreign keys.

  4. From conv_database_design
     When should I use NoSQL over SQL?

  5. From conv_performance
     How can I optimize database queries?

=== Step 4: Generating Context-Aware Response ===
âœ“ AI Response (with context from 3 conversations):

Writing memory-safe, high-performance database code involves several principles and techniques:

1. **Choose the Right Database Type**: Based on previous conversations, we know that different types of databases (NoSQL vs SQL) are suited for different types of tasks. NoSQL is better for flexible schemas, horizontal scaling, high write throughput, or when working with unstructured data. SQL is better for complex queries, ACID transactions, and structured data with well-defined relationships. So, choosing the right database based on your application requirements can lead to better performance.

2. **Normalization**: Normalize data to reduce redundancy and improve data integrity. As mentioned before, this involves dividing large tables into smaller ones and defining relationships between them using foreign keys. However, keep in mind that over-normalization can also lead to performance issues due to join operations.

3. **Optimize Queries**: Write efficient SQL queries to speed up database performance. This can involve techniques like:
   - Using indexes on columns that are frequently queried.
   - Avoiding SELECT * and instead specifying the columns you need.
   - Using JOINs wisely and avoiding Cartesian products.
   - Using LIMIT to restrict the amount of data that your query returns.

4. **Memory Management**: In terms of ensuring memory safety, it depends on the language you're using to interact with the database. If you're using a language like C++, it's important to properly manage memory to avoid leaks. In languages like Java and Python, the garbage collector manages memory, but you should still be mindful of creating unnecessary objects.

5. **Connection Pooling**: Creating a new connection for every database operation is resource-intensive. Connection pooling allows you to reuse existing connections, reducing the overhead of establishing a new connection every time.

6. **Caching**: Implementing a caching system can drastically improve the performance of your database by keeping frequently accessed data in a cache, reducing the need for database calls.

Remember, the best methods for optimizing your database will depend on your specific use-case, data, and system architecture.

=== Step 5: Storing New Conversation ===
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 58 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 414.796209ms
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 2221 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 492.418375ms
    â€¢ Function auto-cleaned up by client
âœ“ New conversation stored and indexed for future retrieval

=== Step 6: Cross-Conversation Search ===
Searching for messages about 'ownership' across ALL conversations...


â†’ Executing text_search()...
  â€¢ Collection: rag_messages
  â€¢ Query: "ownership system"
  â€¢ Limit: 3 results
  â€¢ Search method: Full-text with fuzzy matching & stemming
  â€¢ No vector embeddings needed - pure keyword search
  âœ“ Text search completed in 53.751958ms

âœ“ Found 3 messages mentioning ownership:
  1. From conv_rust_programming: Rust's key features include: memory safety without garbage collection, zero-cost abstractions, ownership system, powerful type system, and excellent concurrency support.

  2. From conv_performance: Rust's ownership system provides zero-cost memory management. Use Box for heap allocation, Rc/Arc for shared ownership, and avoid cloning large data structures. The compiler optimizes away unnecessary allocations.

  3. From conv_new_question: Writing memory-safe, high-performance database code involves several principles and techniques:

1. **Choose the Right Database Type**: Based on previous conversations, we know that different types of databases (NoSQL vs SQL) are suited for different types of tasks. NoSQL is better for flexible schemas, horizontal scaling, high write throughput, or when working with unstructured data. SQL is better for complex queries, ACID transactions, and structured data with well-defined relationships. So, choosing the right database based on your application requirements can lead to better performance.

2. **Normalization**: Normalize data to reduce redundancy and improve data integrity. As mentioned before, this involves dividing large tables into smaller ones and defining relationships between them using foreign keys. However, keep in mind that over-normalization can also lead to performance issues due to join operations.

3. **Optimize Queries**: Write efficient SQL queries to speed up database performance. This can involve techniques like:
   - Using indexes on columns that are frequently queried.
   - Avoiding SELECT * and instead specifying the columns you need.
   - Using JOINs wisely and avoiding Cartesian products.
   - Using LIMIT to restrict the amount of data that your query returns.

4. **Memory Management**: In terms of ensuring memory safety, it depends on the language you're using to interact with the database. If you're using a language like C++, it's important to properly manage memory to avoid leaks. In languages like Java and Python, the garbage collector manages memory, but you should still be mindful of creating unnecessary objects.

5. **Connection Pooling**: Creating a new connection for every database operation is resource-intensive. Connection pooling allows you to reuse existing connections, reducing the overhead of establishing a new connection every time.

6. **Caching**: Implementing a caching system can drastically improve the performance of your database by keeping frequently accessed data in a cache, reducing the need for database calls.

Remember, the best methods for optimizing your database will depend on your specific use-case, data, and system architecture.

=== System Statistics ===

â†’ Querying database statistics...
  â€¢ Using find_all() helper - simplified query API

ğŸ“Š Database Statistics:
  â€¢ Total conversations: 4
  â€¢ Total messages stored: 14
  â€¢ All messages indexed for vector search âœ“
  â€¢ All messages indexed for text search âœ“
  â€¢ All messages queryable by metadata âœ“

=== Step 8: Dynamic Search Configuration ===
Each conversation can have its own search config...

ğŸ’¡ Conversations can store custom search configurations:
  â€¢ Search type: hybrid, text, or vector
  â€¢ Relevance thresholds
  â€¢ Filter by tags or metadata
  â€¢ Collection-specific settings
  â€¢ Per-conversation AI behavior

This enables context-aware search tuned to each conversation's needs!


=== Cleanup ===
Deleting example collections...

âœ… All done! RAG system demonstrated successfully.

âœ“ Using search results to enhance AI responses (RAG)
âœ“ Building a self-improving knowledge base
âœ“ Dynamic search configurations per conversation

ekoDB provides everything needed for AI-powered applications:
  â€¢ Vector search (semantic similarity)
  â€¢ Text search (keyword matching)
  â€¢ Hybrid search (best of both)
  â€¢ AI functions (Chat, Embed)
  â€¢ Flexible querying and filtering
  â€¢ All in one database - no external dependencies!


[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m
[36mRunning Python RAG Example...[0m
[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m
=== ekoDB RAG Conversation System ===

This example shows how ekoDB can power a self-improving AI system
that learns from its own conversation history.

=== Step 1: Building Conversation History ===
Storing previous conversations with embeddings...

  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 34 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.249s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 169 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 1.024s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 33 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.556s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 230 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.459s
    â€¢ Function auto-cleaned up by client
âœ“ Stored Rust programming conversation (4 messages)
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 31 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.229s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 217 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.221s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 33 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.283s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 232 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.559s
    â€¢ Function auto-cleaned up by client
âœ“ Stored database design conversation (4 messages)
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 36 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.254s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 178 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.317s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 37 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.294s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 213 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.274s
    â€¢ Function auto-cleaned up by client
âœ“ Stored performance optimization conversation (4 messages)

=== Step 2: New User Question with Context Retrieval ===
User asks: "How do I write memory-safe high-performance database code?"

=== Step 3: Searching Related Context ===
Using hybrid search to find relevant messages from all conversations...


â†’ Generating embedding for user question...
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 58 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.292s
    â€¢ Function auto-cleaned up by client

â†’ Executing hybrid_search()...
  â€¢ Collection: rag_messages
  â€¢ Query text: "How do I write memory-safe high-performance database code?"
  â€¢ Vector dimensions: 1536
  â€¢ Limit: 5 results
  â€¢ Search type: Semantic (vector) + Keyword (text)
  â€¢ Server combines both scores for relevance ranking
  âœ“ Search completed in 0.090s

âœ“ Found 5 related messages across all conversations:
  1. [Score: 0.000] From conv_database_design
     Use NoSQL when you need: flexible schemas, horizontal scaling, high write throughput, or when working with unstructured data. SQL is better for complex queries, ACID transactions, and structured data with well-defined relationships.

  2. [Score: 0.000] From conv_database_design
     What is database normalization?

  3. [Score: 0.000] From conv_database_design
     Database normalization is the process of organizing data to reduce redundancy and improve data integrity. It involves dividing large tables into smaller ones and defining relationships between them using foreign keys.

  4. [Score: 0.000] From conv_database_design
     When should I use NoSQL over SQL?

  5. [Score: 0.000] From conv_performance
     How can I optimize database queries?

=== Step 4: Generating Context-Aware Response ===
âœ“ AI Response (with context from 3 conversations):

Writing memory-safe high-performance database code involves a range of good practices, here are some of them:

1. **Normalization:** As defined in context 3, normalization is a process that helps reduce data redundancy and improve data integrity. Proper normalization can help optimize the speed and efficiency of most of your database operations.

2. **Optimize Queries:** The performance of a database is often limited by the efficiency of the queries it runs. You can optimize queries by using indexing, avoiding unnecessary columns in SELECT statements, limiting the use of wildcards, and using JOINs wisely.

3. **Choose the Right Database Type:** Depending on your needs, either SQL or NoSQL may be more appropriate (as described in context 1 and 4). SQL databases are good for structured data with complex queries and ACID transactions, while NoSQL databases are better for flexible schemas, horizontal scaling, high write throughput, and unstructured data.

4. **Use Prepared Statements:** Prepared Statements are used to execute the same or similar database statements repeatedly with high efficiency. They also help avoid SQL injection attacks, making your code more secure.

5. **Memory Management:** Be careful with memory allocation and deallocation. Unnecessary memory usage can lead to memory leaks which can slow down your database performance. Make sure to free up memory that is no longer needed.

6. **Concurrency Control:** Efficient handling of multiple simultaneous operations can significantly improve performance in a multi-user and multi-transaction environment. Techniques can include locking, multiversion concurrency control (MVCC), and transaction isolation levels.

7. **Caching:** Implement caching strategies to store frequently accessed data in memory for faster retrieval.

8. **Batch Processing:** Instead of writing or updating data one record at a time, consider batch processing. This can be much faster for large amounts of data.

Remember, the specifics of how you implement these will depend on the specific database system you are using.

=== Step 5: Storing New Conversation ===
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 58 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 1.438s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 2079 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.186s
    â€¢ Function auto-cleaned up by client
âœ“ New conversation stored and indexed for future retrieval

=== Step 6: Cross-Conversation Search ===
Searching for messages about 'ownership' across ALL conversations...

âœ“ Found 3 messages mentioning ownership:
  1. From conv_rust_programming: Rust's key features include: memory safety without garbage collection, zero-cost abstractions, ownership system, powerful type system, and excellent concurrency support.

  2. From conv_performance: Rust's ownership system provides zero-cost memory management. Use Box for heap allocation, Rc/Arc for shared ownership, and avoid cloning large data structures. The compiler optimizes away unnecessary allocations.

  3. From conv_rust_programming: The borrow checker enforces Rust's ownership rules at compile time. It ensures that references don't outlive the data they point to and prevents data races by allowing either multiple immutable references or one mutable reference.

=== System Statistics ===
Total conversations: 4
Total messages stored: 14
All messages are indexed for vector search âœ“
All messages are indexed for text search âœ“
All messages are queryable by metadata âœ“

=== Step 8: Dynamic Search Configuration ===
Each conversation can have its own search config...

ğŸ’¡ Conversations can store custom search configurations:
  â€¢ Search type: hybrid, text, or vector
  â€¢ Relevance thresholds
  â€¢ Filter by tags or metadata
  â€¢ Collection-specific settings
  â€¢ Per-conversation AI behavior

This enables context-aware search tuned to each conversation's needs!

=== Cleanup ===
âœ“ Cleanup complete


=== ğŸ“š Summary: What This Example Showed ===

ğŸ”§ ekoDB Native Capabilities Used:
  âœ“ Functions with Embed operation (AI integration)
  âœ“ Hybrid Search (text + vector combined)
  âœ“ Text Search (full-text with stemming)
  âœ“ Automatic embedding generation
  âœ“ Cross-collection queries

ğŸš€ New Client Helper Methods:
  â€¢ client.embed(text, model) - Generate embeddings
  â€¢ client.hybrid_search() - Semantic + keyword search
  â€¢ client.text_search() - Full-text search
  â€¢ client.find_all() - Query all documents

ğŸ’¡ Key Takeaways:
  1. ekoDB handles AI Functions natively - no external services needed
  2. One-line embedding generation with auto-cleanup
  3. Hybrid search combines semantic understanding + keyword matching
  4. Perfect for RAG: store, search, and retrieve context
  5. All AI capabilities accessible through simple client methods

ğŸ¯ Build production RAG systems with ekoDB!
   â†’ Set OPENAI_API_KEY in your ekoDB server environment
   â†’ Use these client helpers to make AI integration simple
   â†’ Scale to millions of documents with native indexing


[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m
[36mRunning TypeScript RAG Example...[0m
[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m
=== ekoDB RAG Conversation System ===

This example shows how ekoDB can power a self-improving AI system
that learns from its own conversation history.

=== Step 1: Building Conversation History ===
Storing previous conversations with embeddings...

  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 34 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.209s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 169 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.408s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 33 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.231s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 230 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.272s
    â€¢ Function auto-cleaned up by client
âœ“ Stored Rust programming conversation (4 messages)
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 31 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.302s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 217 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.222s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 33 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.303s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 232 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.443s
    â€¢ Function auto-cleaned up by client
âœ“ Stored database design conversation (4 messages)
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 36 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.261s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 178 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.298s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 37 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.204s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 213 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.223s
    â€¢ Function auto-cleaned up by client
âœ“ Stored performance optimization conversation (4 messages)

=== Step 2: New User Question with Context Retrieval ===
User asks: "How do I write memory-safe high-performance database code?"

=== Step 3: Searching Related Context ===
Using hybrid search to find relevant messages from all conversations...


â†’ Generating embedding for user question...
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 58 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.201s
    â€¢ Function auto-cleaned up by client

â†’ Executing hybridSearch()...
  â€¢ Collection: rag_messages
  â€¢ Query text: "How do I write memory-safe high-performance database code?"
  â€¢ Vector dimensions: 1536
  â€¢ Limit: 5 results
  â€¢ Search type: Semantic (vector) + Keyword (text)
  â€¢ Server combines both scores for relevance ranking
  âœ“ Search completed in 0.082s
âœ“ Found 5 related messages across all conversations:
  1. [Score: 0.000] From conv_database_design
     Database normalization is the process of organizing data to reduce redundancy and improve data integrity. It involves dividing large tables into smaller ones and defining relationships between them using foreign keys.

  2. [Score: 0.000] From conv_database_design
     What is database normalization?

  3. [Score: 0.000] From conv_database_design
     Use NoSQL when you need: flexible schemas, horizontal scaling, high write throughput, or when working with unstructured data. SQL is better for complex queries, ACID transactions, and structured data with well-defined relationships.

  4. [Score: 0.000] From conv_database_design
     When should I use NoSQL over SQL?

  5. [Score: 0.000] From conv_performance
     How can I optimize database queries?

=== Step 4: Generating Context-Aware Response ===
âœ“ AI Response (with context from 3 conversations):

Writing memory-safe, high-performance database code involves several strategies:

1. **Use Efficient Queries**: As discussed in Context 5, optimizing your queries is crucial for high performance. Avoid using SELECT * queries; instead, specify the exact fields you need. Use JOINs sparingly and wisely, as they can be expensive. Index your tables appropriately based on the fields you most commonly query on.

2. **Normalize Your Database**: Referencing Context 1, database normalization can help reduce redundancy and improve data integrity, which can lead to better performance. It makes your database leaner and more efficient by ensuring data is not duplicated unnecessarily.

3. **Choose the Right Database**: Depending on your needs, the choice between SQL and NoSQL (as discussed in Context 3 and 4) can impact performance. SQL databases are great for structured data and complex queries, while NoSQL databases are more suited for unstructured data, high write throughput, and horizontal scaling.

4. **Manage Connections Efficiently**: Connections to databases are expensive to create and maintain. Use connection pooling to reuse existing connections rather than establishing a new one every time.

5. **Memory Management**: In terms of memory safety, it's crucial to avoid memory leaks in your code. This can be achieved by proper resource allocation and deallocation, and by utilizing garbage collection where applicable. 

6. **Use Prepared Statements**: These can not only improve performance by reducing the time needed to execute the same query multiple times, but they also help prevent SQL injection attacks, thereby improving the safety of your code.

7. **Use Batch Processing**: If you need to perform a large number of similar operations (like INSERTs or UPDATEs), doing them in a batch rather than individually can significantly improve performance.

Remember, the best practices can vary depending on the specific database system (like MySQL, PostgreSQL, MongoDB, etc.) and the programming language you're using. Always refer to the relevant documentation for the best practices.

=== Step 5: Storing New Conversation ===
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 58 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.300s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 2101 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.351s
    â€¢ Function auto-cleaned up by client
âœ“ New conversation stored and indexed for future retrieval

=== Step 6: Cross-Conversation Search ===
Searching for messages about 'ownership' across ALL conversations...


â†’ Executing textSearch()...
  â€¢ Collection: rag_messages
  â€¢ Query: "ownership system"
  â€¢ Limit: 3 results
  â€¢ Search method: Full-text with fuzzy matching & stemming
  â€¢ No vector embeddings needed - pure keyword search
  âœ“ Text search completed in 0.043s
âœ“ Found 3 messages mentioning ownership:
  1. From conv_performance: Rust's ownership system provides zero-cost memory management. Use Box for heap allocation, Rc/Arc for shared ownership, and avoid cloning large data structures. The compiler optimizes away unnecessary allocations.

  2. From conv_rust_programming: Rust's key features include: memory safety without garbage collection, zero-cost abstractions, ownership system, powerful type system, and excellent concurrency support.

  3. From conv_rust_programming: The borrow checker enforces Rust's ownership rules at compile time. It ensures that references don't outlive the data they point to and prevents data races by allowing either multiple immutable references or one mutable reference.

=== System Statistics ===

â†’ Querying database statistics...
  â€¢ Using findAllWithLimit() helper - simplified query API

ğŸ“Š Database Statistics:
  â€¢ Total conversations: 4
  â€¢ Total messages stored: 14
  â€¢ All messages indexed for vector search âœ“
  â€¢ All messages indexed for text search âœ“
  â€¢ All messages queryable by metadata âœ“

=== Step 8: Dynamic Search Configuration ===
Each conversation can have its own search config...

ğŸ’¡ Conversations can store custom search configurations:
  â€¢ Search type: hybrid, text, or vector
  â€¢ Relevance thresholds
  â€¢ Filter by tags or metadata
  â€¢ Collection-specific settings
  â€¢ Per-conversation AI behavior

This enables context-aware search tuned to each conversation's needs!

=== Cleanup ===
âœ“ Cleanup complete


=== ğŸ“š Summary: What This Example Showed ===

ğŸ”§ ekoDB Native Capabilities Used:
  âœ“ Functions with Embed operation (AI integration)
  âœ“ Hybrid Search (text + vector combined)
  âœ“ Text Search (full-text with stemming)
  âœ“ Automatic embedding generation
  âœ“ Cross-collection queries

ğŸš€ New Client Helper Methods:
  â€¢ client.embed(text, model) - Generate embeddings
  â€¢ client.hybridSearch() - Semantic + keyword search
  â€¢ client.textSearch() - Full-text search
  â€¢ client.findAllWithLimit() - Query all documents

ğŸ’¡ Key Takeaways:
  1. ekoDB handles AI Functions natively - no external services needed
  2. One-line embedding generation with auto-cleanup
  3. Hybrid search combines semantic understanding + keyword matching
  4. Perfect for RAG: store, search, and retrieve context
  5. All AI capabilities accessible through simple client methods

ğŸ¯ Build production RAG systems with ekoDB!
   â†’ Set OPENAI_API_KEY in your ekoDB server environment
   â†’ Use these client helpers to make AI integration simple
   â†’ Scale to millions of documents with native indexing


[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m
[36mRunning Go RAG Example...[0m
[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m
=== ekoDB RAG Conversation System ===

This example shows how ekoDB can power a self-improving AI system
that learns from its own conversation history.

=== Step 1: Building Conversation History ===
Storing previous conversations with embeddings...

  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 34 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.276s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 169 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.300s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 33 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.318s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 230 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.505s
    â€¢ Function auto-cleaned up by client
âœ“ Stored Rust programming conversation (4 messages)
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 31 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.223s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 217 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.228s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 33 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.228s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 232 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.277s
    â€¢ Function auto-cleaned up by client
âœ“ Stored database design conversation (4 messages)
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 36 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.423s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 178 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.715s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 37 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.251s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 213 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.257s
    â€¢ Function auto-cleaned up by client
âœ“ Stored performance optimization conversation (4 messages)

=== Step 2: New User Question with Context Retrieval ===
User asks: "How do I write memory-safe high-performance database code?"

=== Step 3: Searching Related Context ===
Using hybrid search to find relevant messages from all conversations...


â†’ Generating embedding for user question...
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 58 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.350s
    â€¢ Function auto-cleaned up by client

â†’ Executing HybridSearch()...
  â€¢ Collection: rag_messages
  â€¢ Query text: "How do I write memory-safe high-performance database code?"
  â€¢ Vector dimensions: 1536
  â€¢ Limit: 5 results
  â€¢ Search type: Semantic (vector) + Keyword (text)
  â€¢ Server combines both scores for relevance ranking
  âœ“ Search completed in 0.091s

âœ“ Found 5 related messages across all conversations:
  1. [Score: 0.000] From conv_database_design
     Database normalization is the process of organizing data to reduce redundancy and improve data integrity. It involves dividing large tables into smaller ones and defining relationships between them using foreign keys.

  2. [Score: 0.000] From conv_database_design
     What is database normalization?

  3. [Score: 0.000] From conv_database_design
     Use NoSQL when you need: flexible schemas, horizontal scaling, high write throughput, or when working with unstructured data. SQL is better for complex queries, ACID transactions, and structured data with well-defined relationships.

  4. [Score: 0.000] From conv_database_design
     When should I use NoSQL over SQL?

  5. [Score: 0.000] From conv_performance
     How can I optimize database queries?

=== Step 4: Generating Context-Aware Response ===
âœ“ AI Response (with context from 3 conversations):

Writing memory-safe, high-performance database code involves several key considerations:

1. **Optimize your Queries**: As mentioned in Context 5, optimizing database queries is crucial for high performance. Use indexing appropriately, avoid full table scans, limit the data you're fetching, and write efficient joins. Also, use the EXPLAIN command to understand how your queries are being executed and where you can make improvements.

2. **Normalization**: As noted in Context 1 and 2, database normalization reduces redundancy and improves data integrity. It can also enhance the performance of some types of queries. 

3. **Choose the Right Database Type**: As pointed out in Context 3 and 4, choosing between SQL and NoSQL depends on your specific needs. SQL databases are better for complex queries and structured data, while NoSQL databases excel at handling unstructured data, scaling horizontally, and providing high write throughput.

4. **Memory Management**: Ensure your code is memory-safe by properly managing your database connections. Make sure to close connections when they're no longer needed to free up resources. Also, use appropriate data structures to store data in memory and avoid memory leaks.

5. **Concurrency Control**: Use transactions where necessary to ensure data consistency and integrity. SQL databases follow ACID properties (Atomicity, Consistency, Isolation, Durability) as mentioned in Context 3.

6. **Caching**: Cache frequently accessed data to reduce the load on your database and increase performance. But, be careful to manage cache invalidation correctly to ensure data consistency.

7. **Batch Processing**: If you are writing or reading large amounts of data, it might be more efficient to do it in batches rather than one record at a time.

8. **Monitoring and Profiling**: Regularly monitor and profile your database performance. This will help you identify slow queries or memory issues and fix them proactively.

9. **Use Prepared Statements**: Prepared statements not only help in preventing SQL injections but can also improve performance as they're precompiled and reusable.

Remember, each application has unique requirements, so there are no one-size-fits-all solutions. Always profile and test different approaches to find what works best for your specific situation.

=== Step 5: Storing New Conversation ===
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 58 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.319s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB Embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 2325 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.325s
    â€¢ Function auto-cleaned up by client
âœ“ New conversation stored and indexed for future retrieval

=== Step 6: Cross-Conversation Search ===
Searching for messages about 'ownership' across ALL conversations...


â†’ Executing TextSearch()...
  â€¢ Collection: rag_messages
  â€¢ Query: "ownership system"
  â€¢ Limit: 3 results
  â€¢ Search method: Full-text with fuzzy matching & stemming
  â€¢ No vector embeddings needed - pure keyword search
  âœ“ Text search completed in 0.041s

âœ“ Found 3 messages mentioning ownership:
  1. From conv_rust_programming: Rust's key features include: memory safety without garbage collection, zero-cost abstractions, ownership system, powerful type system, and excellent concurrency support.

  2. From conv_performance: Rust's ownership system provides zero-cost memory management. Use Box for heap allocation, Rc/Arc for shared ownership, and avoid cloning large data structures. The compiler optimizes away unnecessary allocations.

  3. From conv_rust_programming: The borrow checker enforces Rust's ownership rules at compile time. It ensures that references don't outlive the data they point to and prevents data races by allowing either multiple immutable references or one mutable reference.

=== System Statistics ===

â†’ Querying database statistics...
  â€¢ Using FindAll() helper - simplified query API

ğŸ“Š Database Statistics:
  â€¢ Total conversations: 4
  â€¢ Total messages stored: 14
  â€¢ All messages indexed for vector search âœ“
  â€¢ All messages indexed for text search âœ“
  â€¢ All messages queryable by metadata âœ“

=== Step 8: Dynamic Search Configuration ===
Each conversation can have its own search config...

ğŸ’¡ Conversations can store custom search configurations:
  â€¢ Search type: hybrid, text, or vector
  â€¢ Relevance thresholds
  â€¢ Filter by tags or metadata
  â€¢ Collection-specific settings
  â€¢ Per-conversation AI behavior

This enables context-aware search tuned to each conversation's needs!

=== Cleanup ===
âœ“ Cleanup complete


=== ğŸ“š Summary: What This Example Showed ===

ğŸ”§ ekoDB Native Capabilities Used:
  âœ“ Functions with Embed operation (AI integration)
  âœ“ Hybrid Search (text + vector combined)
  âœ“ Text Search (full-text with stemming)
  âœ“ Automatic embedding generation
  âœ“ Cross-collection queries

ğŸš€ New Client Helper Methods:
  â€¢ client.Embed(text, model) - Generate embeddings
  â€¢ client.HybridSearch() - Semantic + keyword search
  â€¢ client.TextSearch() - Full-text search
  â€¢ client.FindAll() - Query all documents

ğŸ’¡ Key Takeaways:
  1. ekoDB handles AI Functions natively - no external services needed
  2. One-line embedding generation with auto-cleanup
  3. Hybrid search combines semantic understanding + keyword matching
  4. Perfect for RAG: store, search, and retrieve context
  5. All AI capabilities accessible through simple client methods

ğŸ¯ Build production RAG systems with ekoDB!
   â†’ Set OPENAI_API_KEY in your ekoDB server environment
   â†’ Use these client helpers to make AI integration simple
   â†’ Scale to millions of documents with native indexing


[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m
[36mRunning Kotlin RAG Example...[0m
[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m
=== ekoDB RAG Conversation System ===

This example shows how ekoDB can power a self-improving AI system
that learns from its own conversation history.

SLF4J(W): No SLF4J providers were found.
SLF4J(W): Defaulting to no-operation (NOP) logger implementation
SLF4J(W): See https://www.slf4j.org/codes.html#noProviders for further details.
=== Step 1: Building Conversation History ===
Storing previous conversations with embeddings...

  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 34 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.353s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 169 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.248s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 33 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.258s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 230 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.26s
    â€¢ Function auto-cleaned up by client
âœ“ Stored Rust programming conversation (4 messages)
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 31 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.361s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 217 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.326s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 33 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.261s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 232 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.282s
    â€¢ Function auto-cleaned up by client
âœ“ Stored database design conversation (4 messages)
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 36 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.468s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 178 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.309s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 37 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.281s
    â€¢ Function auto-cleaned up by client
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 213 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.242s
    â€¢ Function auto-cleaned up by client
âœ“ Stored performance optimization conversation (4 messages)

=== Step 2: New User Question with Context Retrieval ===
User asks: "How do I write memory-safe high-performance database code?"

=== Step 3: Searching Related Context ===
Using hybrid search to find relevant messages from all conversations...


â†’ Generating embedding for user question...
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 58 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.308s
    â€¢ Function auto-cleaned up by client

â†’ Executing hybridSearch()...
  â€¢ Collection: rag_messages
  â€¢ Query text: "How do I write memory-safe high-performance database code?"
  â€¢ Vector dimensions: 1536
  â€¢ Limit: 5 results
  â€¢ Search type: Semantic (vector) + Keyword (text)
  â€¢ Server combines both scores for relevance ranking
  âœ“ Search completed in 0.088s

âœ“ Found 5 related messages across all conversations:
  1. [Score: 0.000] From conv_performance
     How can I optimize database queries?

  2. [Score: 0.000] From conv_database_design
     Use NoSQL when you need: flexible schemas, horizontal scaling, high write throughput, or when working with unstructured data. SQL is better for complex queries, ACID transactions, and structured data with well-defined relationships.

  3. [Score: 0.000] From conv_database_design
     When should I use NoSQL over SQL?

  4. [Score: 0.000] From conv_database_design
     What is database normalization?

  5. [Score: 0.000] From conv_database_design
     Database normalization is the process of organizing data to reduce redundancy and improve data integrity. It involves dividing large tables into smaller ones and defining relationships between them using foreign keys.

=== Step 4: Generating Context-Aware Response ===
âœ“ Context prepared from search results
âœ“ AI would use this context to generate comprehensive response

=== Step 5: Storing New Conversation ===
  â†’ Calling ekoDB embed() helper...
    â€¢ Using model: text-embedding-3-small
    â€¢ Text length: 58 characters
    â€¢ Behind the scenes: Creating temp Function with Embed operation
    âœ“ Generated embedding: 1536 dimensions in 0.221s
    â€¢ Function auto-cleaned up by client
âœ“ New conversation stored and indexed for future retrieval

=== Step 6: Cross-Conversation Search ===
Searching for messages about 'ownership' across ALL conversations...


â†’ Executing textSearch()...
  â€¢ Collection: rag_messages
  â€¢ Query: "ownership system"
  â€¢ Limit: 3 results
  â€¢ Search method: Full-text with fuzzy matching & stemming
  â€¢ No vector embeddings needed - pure keyword search
  âœ“ Text search completed in 0.044s

âœ“ Found 3 messages mentioning ownership:
  1. From conv_rust_programming: Rust's key features include: memory safety without garbage collection, zero-cost abstractions, ownership system, powerful type system, and excellent concurrency support.

  2. From conv_performance: Rust's ownership system provides zero-cost memory management. Use Box for heap allocation, Rc/Arc for shared ownership, and avoid cloning large data structures. The compiler optimizes away unnecessary allocations.

  3. From conv_rust_programming: The borrow checker enforces Rust's ownership rules at compile time. It ensures that references don't outlive the data they point to and prevents data races by allowing either multiple immutable references or one mutable reference.

=== System Statistics ===

â†’ Querying database statistics...
  â€¢ Using findAllWithLimit() helper - simplified query API

ğŸ“Š Database Statistics:
  â€¢ Total conversations: 4
  â€¢ Total messages stored: 13
  â€¢ All messages indexed for vector search âœ“
  â€¢ All messages indexed for text search âœ“
  â€¢ All messages queryable by metadata âœ“

=== Step 8: Dynamic Search Configuration ===
Each conversation can have its own search config...

ğŸ’¡ Conversations can store custom search configurations:
  â€¢ Search type: hybrid, text, or vector
  â€¢ Relevance thresholds
  â€¢ Filter by tags or metadata
  â€¢ Collection-specific settings
  â€¢ Per-conversation AI behavior

This enables context-aware search tuned to each conversation's needs!

=== Cleanup ===
âœ“ Cleanup complete


=== ğŸ“š Summary: What This Example Showed ===

ğŸ”§ ekoDB Native Capabilities Used:
  âœ“ Functions with Embed operation (AI integration)
  âœ“ Hybrid Search (text + vector combined)
  âœ“ Text Search (full-text with stemming)
  âœ“ Automatic embedding generation
  âœ“ Cross-collection queries

ğŸš€ New Client Helper Methods:
  â€¢ client.embed(text, model) - Generate embeddings
  â€¢ client.hybridSearch() - Semantic + keyword search
  â€¢ client.textSearch() - Full-text search
  â€¢ client.findAllWithLimit() - Query all documents

ğŸ’¡ Key Takeaways:
  1. ekoDB handles AI Functions natively - no external services needed
  2. One-line embedding generation with auto-cleanup
  3. Hybrid search combines semantic understanding + keyword matching
  4. Perfect for RAG: store, search, and retrieve context
  5. All AI capabilities accessible through simple client methods

ğŸ¯ Build production RAG systems with ekoDB!
   â†’ Set OPENAI_API_KEY in your ekoDB server environment
   â†’ Use these client helpers to make AI integration simple
   â†’ Scale to millions of documents with native indexing


[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m
âœ… [32mRAG Examples Complete![0m
[36mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m

[32mWhat you just saw across 5 languages:[0m
  âœ“ Embeddings generated via ekoDB Functions
  âœ“ Hybrid search (semantic + keyword)
  âœ“ Text search with stemming
  âœ“ Cross-conversation context retrieval
  âœ“ Simple client helpers wrapping powerful AI

[36mMission: AI for All ğŸš€[0m - Making RAG accessible to everyone!

